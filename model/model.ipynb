{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outstanding-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import jsondim\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import *\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMMax(nn.Module):\n",
    "#     def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "#         super(LSTMMax, self).__init__()\n",
    "#         self.embed_size=embed_size\n",
    "#         self.rnn_size=rnn_size\n",
    "#         self.cond_vocab_size=cond_vocab_size\n",
    "#         self.cond_seq_len=cond_seq_len\n",
    "#         self.proc_vocab_size=proc_vocab_size\n",
    "#         self.proc_seq_len=proc_seq_len\n",
    "#         self.med_vocab_size=med_vocab_size\n",
    "#         self.med_seq_len=med_seq_len\n",
    "#         self.lab_vocab_size=lab_vocab_size\n",
    "#         self.lab_seq_len=lab_seq_len\n",
    "#         self.time_vocab_size=time_vocab_size\n",
    "#         self.census_vocab_size=census_vocab_size\n",
    "#         self.batch_size=batch_size\n",
    "#         self.padding_idx = 0\n",
    "#         self.modalities=5\n",
    "#         self.device=device\n",
    "#         self.build()\n",
    "        \n",
    "#     def build(self):\n",
    "#         self.condEmbed=nn.Embedding(self.cond_vocab_size,self.embed_size,self.padding_idx)\n",
    "#         self.procEmbed=nn.Embedding(self.proc_vocab_size,self.embed_size,self.padding_idx)\n",
    "#         self.medEmbed=nn.Embedding(self.med_vocab_size,self.embed_size,self.padding_idx)\n",
    "#         self.labEmbed=nn.Embedding(self.lab_vocab_size,self.embed_size,self.padding_idx)\n",
    "#         self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)\n",
    "        \n",
    "#         self.condRnn = nn.LSTM(input_size=self.embed_size*3,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "#         self.procRnn = nn.LSTM(input_size=self.embed_size*3,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "#         self.medRnn = nn.LSTM(input_size=self.embed_size*3,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "#         self.labRnn = nn.LSTM(input_size=self.embed_size*3,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        \n",
    "#         self.cond_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "#         self.proc_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "#         self.med_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "#         self.lab_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "#         self.census_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "        \n",
    "#         self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "#         self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "#         self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "#         self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "#         self.census_fc=nn.Linear(self.embed_size, 1, False)\n",
    "        \n",
    "#         #self.fc=nn.Linear(self.modalities, 1, False)\n",
    "        \n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.timeEmbed=nn.Embedding(self.time_vocab_size,self.embed_size,self.padding_idx)\n",
    "#         self.cond_drop = nn.Dropout(0.2)\n",
    "#         self.proc_drop = nn.Dropout(0.4)\n",
    "#         self.med_drop = nn.Dropout(0.4)\n",
    "#         self.lab_drop = nn.Dropout(0.4)\n",
    "#         #self.census_drop = nn.Dropout()\n",
    "        \n",
    "#     def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, lab_lengths,census,contrib):\n",
    "        \n",
    "#         #print(conds.shape)\n",
    "#         cond_time=conds[1]\n",
    "#         conds=conds[0]\n",
    "#         proc_time=procs[1]\n",
    "#         procs=procs[0]\n",
    "#         med_time=meds[1]\n",
    "#         meds=meds[0]\n",
    "#         lab_time=labs[1]\n",
    "#         labs=labs[0]\n",
    "        \n",
    "        \n",
    "#         #print(cond_time[0])\n",
    "#         #cond_time=np.diff(cond_time)\n",
    "#         #print(cond_time[0])\n",
    "#         #print(torch.diff(cond_time,dim=1)[0])\n",
    "#         #print(time.shape)\n",
    "#         #initialize hidden and cell state\n",
    "#         h_0, c_0 = self.init_hidden()\n",
    "#         #print(\"h\",h_0.shape)\n",
    "        \n",
    "#         h_0, c_0, cond_time, conds = h_0.to(self.device), c_0.to(self.device),cond_time.to(self.device),conds.to(self.device)\n",
    "#         proc_time, procs =proc_time.to(self.device),procs.to(self.device)\n",
    "#         med_time, meds =med_time.to(self.device),meds.to(self.device)\n",
    "#         lab_time, labs =lab_time.to(self.device),labs.to(self.device)\n",
    "#         census=census.to(self.device)\n",
    "        \n",
    "#         #Embedd all sequences\n",
    "#         #print(\"embedded\")\n",
    "#         conds=self.condEmbed(conds)\n",
    "#         cond_time=self.timeEmbed(cond_time)\n",
    "#         cond_time_sin=torch.sin(cond_time)\n",
    "#         cond_time_cos=torch.cos(cond_time)\n",
    "#         #print(cond_time.shape)\n",
    "#         #print(conds.shape)\n",
    "#         procs=self.procEmbed(procs)\n",
    "#         #print(\"time\")\n",
    "#         proc_time=self.timeEmbed(proc_time)\n",
    "#         proc_time_sin=torch.sin(proc_time)\n",
    "#         proc_time_cos=torch.cos(proc_time)\n",
    "#         #print(\"embedded\")\n",
    "#         meds=self.medEmbed(meds)\n",
    "#         med_time=self.timeEmbed(med_time)\n",
    "#         med_time_sin=torch.sin(med_time)\n",
    "#         med_time_cos=torch.cos(med_time)\n",
    "#         #print(\"embedded\")\n",
    "#         labs=self.labEmbed(labs)\n",
    "#         lab_time=self.timeEmbed(lab_time)\n",
    "#         lab_time_sin=torch.sin(lab_time)\n",
    "#         lab_time_cos=torch.cos(lab_time)\n",
    "#         #print(\"embedded\",conds.shape)\n",
    "#         #print(conds[0])\n",
    "#         census=self.censusEmbed(census)\n",
    "#         #print(\"embedded\",census.shape)\n",
    "        \n",
    "        \n",
    "#         conds=torch.cat((conds,cond_time_sin,cond_time_cos),dim=2)\n",
    "#         #print(conds.shape)\n",
    "#         procs=torch.cat((procs,proc_time_sin,proc_time_cos),dim=2)\n",
    "#         #print(procs.shape)\n",
    "#         meds=torch.cat((meds,med_time_sin,med_time_cos),dim=2)\n",
    "#         #print(meds.shape)\n",
    "#         labs=torch.cat((labs,lab_time_sin,lab_time_cos),dim=2)\n",
    "#         #print(labs.shape)\n",
    "#         #print(\"padding\")\n",
    "#         #Pack all sequences\n",
    "#         conds_pack = torch.nn.utils.rnn.pack_padded_sequence(conds, cond_lengths, batch_first=True,enforce_sorted=False)\n",
    "#         #print(\"cond\")\n",
    "#         procs_pack = torch.nn.utils.rnn.pack_padded_sequence(procs, proc_lengths, batch_first=True,enforce_sorted=False)\n",
    "#         #print(\"proc\")\n",
    "#         meds_pack = torch.nn.utils.rnn.pack_padded_sequence(meds, med_lengths, batch_first=True,enforce_sorted=False)\n",
    "#         #print(\"meds\")\n",
    "#         labs_pack = torch.nn.utils.rnn.pack_padded_sequence(labs, lab_lengths, batch_first=True,enforce_sorted=False)\n",
    "#         #print(\"labs\")\n",
    "#         #Run through LSTM\n",
    "#         cond_output, (cond_h_n, cond_c_n)=self.condRnn( conds_pack, (h_0, c_0))\n",
    "#         #print(\"cond\")\n",
    "#         proc_output, (proc_h_n, proc_c_n)=self.procRnn( procs_pack, (h_0, c_0))\n",
    "#         med_output, (med_h_n, med_c_n)=self.medRnn( meds_pack, (h_0, c_0))\n",
    "#         lab_output, (lab_h_n, lab_c_n)=self.labRnn( labs_pack, (h_0, c_0))\n",
    "#         #print(\"h_n\",h_n.shape)\n",
    "       \n",
    "#         #unpack sequence\n",
    "#         cond_output, _ = torch.nn.utils.rnn.pad_packed_sequence(cond_output, batch_first=True)\n",
    "#         proc_output, _ = torch.nn.utils.rnn.pad_packed_sequence(proc_output, batch_first=True)\n",
    "#         med_output, _ = torch.nn.utils.rnn.pad_packed_sequence(med_output, batch_first=True)\n",
    "#         lab_output, _ = torch.nn.utils.rnn.pad_packed_sequence(lab_output, batch_first=True)\n",
    "#         #print(\"upack\")\n",
    "#         cond_output=self.cond_drop(cond_output)\n",
    "#         proc_output=self.proc_drop(proc_output)\n",
    "#         med_output=self.med_drop(med_output)\n",
    "#         lab_output=self.lab_drop(lab_output)\n",
    "        \n",
    "#         cond_output=F.relu(cond_output)\n",
    "#         proc_output=F.relu(proc_output)\n",
    "#         med_output=F.relu(med_output)\n",
    "#         lab_output=F.relu(lab_output)\n",
    "        \n",
    "        \n",
    "#         #print(\"output\",cond_output.shape)\n",
    "#         #print(\"hi\")\n",
    "#         cond_output=cond_output.view(cond_output.shape[0],cond_output.shape[2],cond_output.shape[1])\n",
    "#         proc_output=proc_output.view(proc_output.shape[0],proc_output.shape[2],proc_output.shape[1])\n",
    "#         med_output=med_output.view(med_output.shape[0],med_output.shape[2],med_output.shape[1])\n",
    "#         lab_output=lab_output.view(lab_output.shape[0],lab_output.shape[2],lab_output.shape[1])\n",
    "#         census_output=census.view(census.shape[0],census.shape[2],census.shape[1])\n",
    "#         #print(\"before maxpool\",census_output.shape)\n",
    "#         census_output=F.relu(census_output)\n",
    "        \n",
    "#         cond_pool, cond_indices = self.cond_max(cond_output)\n",
    "#         proc_pool, proc_indices = self.proc_max(proc_output)\n",
    "#         med_pool, med_indices = self.med_max(med_output)\n",
    "#         lab_pool, lab_indices = self.lab_max(lab_output)\n",
    "#         census_pool, census_indices = self.census_max(census_output)\n",
    "#         #print(\"indices\",cond_indices.shape)\n",
    "#         #print(cond_indices[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         cond_pool = torch.squeeze(cond_pool)\n",
    "#         cond_indices = torch.squeeze(cond_indices)\n",
    "#         proc_pool = torch.squeeze(proc_pool)\n",
    "#         proc_indices = torch.squeeze(proc_indices)\n",
    "#         med_pool = torch.squeeze(med_pool)\n",
    "#         med_indices = torch.squeeze(med_indices)\n",
    "#         lab_pool = torch.squeeze(lab_pool)\n",
    "#         lab_indices = torch.squeeze(lab_indices)\n",
    "#         census_pool = torch.squeeze(census_pool)\n",
    "#         census_indices = torch.squeeze(census_indices)\n",
    "#         #print(\"max output\",cond_pool.shape)\n",
    "#         #print(cond_pool[0])\n",
    "#         #cond_output = torch.cat((cond_output,torch.zeros(cond_output.shape[0],int(self.cond_seq_len-cond_output.shape[1]))),1)\n",
    "#         #print(\"max output\",cond_output.shape)\n",
    "#         #print(cond_output[0])\n",
    "        \n",
    "#         cond_out1 = self.cond_fc(cond_pool)\n",
    "#         proc_out1 = self.proc_fc(proc_pool)\n",
    "#         med_out1 = self.med_fc(med_pool)\n",
    "#         lab_out1 = self.lab_fc(lab_pool)\n",
    "#         census_out1 = self.census_fc(census_pool)\n",
    "        \n",
    "#         #print(cond_out1.shape)\n",
    "#         #print(proc_out1.shape)\n",
    "#         #print(med_out1.shape)\n",
    "#         #print(lab_out1.shape)\n",
    "#         #print(census_out1.shape)\n",
    "#         #out1=torch.cat((cond_out1,proc_out1,med_out1,lab_out1,census_out1),1)\n",
    "#         out1=torch.add(cond_out1, proc_out1)\n",
    "#         out1=torch.add(out1, med_out1)\n",
    "#         out1=torch.add(out1, lab_out1)\n",
    "#         out1=torch.add(out1, census_out1)\n",
    "#         #print(out1.shape)\n",
    "#         #out1=self.fc(out1)\n",
    "#         sigout1 = self.sig(out1)\n",
    "#         #print(\"sig out\",out1.shape)\n",
    "#         #print(out1[0])\n",
    "        \n",
    "#         if contrib:\n",
    "#             cond_pool = cond_pool.data.cpu().numpy()\n",
    "#             proc_pool = proc_pool.data.cpu().numpy()\n",
    "#             med_pool = med_pool.data.cpu().numpy()\n",
    "#             lab_pool = lab_pool.data.cpu().numpy()\n",
    "#             census_pool = census_pool.data.cpu().numpy()\n",
    "#             #print(cond_pool.shape)\n",
    "#             assert cond_pool.shape == cond_indices.shape\n",
    "#             cond_fc_weight = self.cond_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "#             proc_fc_weight = self.proc_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "#             med_fc_weight = self.med_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "#             lab_fc_weight = self.lab_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "#             census_fc_weight = self.census_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "#             #print(\"logit_weight\",fc_weight.shape)\n",
    "#             cond_contri = np.zeros((self.batch_size, self.cond_seq_len))\n",
    "#             proc_contri = np.zeros((self.batch_size, self.proc_seq_len))\n",
    "#             med_contri = np.zeros((self.batch_size, self.med_seq_len))\n",
    "#             lab_contri = np.zeros((self.batch_size, self.lab_seq_len))\n",
    "#             census_contri = np.zeros((self.batch_size, self.census_vocab_size))\n",
    "#             #print(\"contrib\",cond_contri.shape)\n",
    "#             for i in range(cond_pool.shape[0]):\n",
    "#                 for j in range(cond_pool.shape[1]):\n",
    "#                     con = cond_pool[i,j] * cond_fc_weight[j]\n",
    "#                     idx = cond_indices[i, j]\n",
    "#                     cond_contri[i,idx] += con\n",
    "#             #print(\"contrib\",cond_contri.shape)        \n",
    "#             for i in range(proc_pool.shape[0]):\n",
    "#                 for j in range(proc_pool.shape[1]):\n",
    "#                     con = proc_pool[i,j] * proc_fc_weight[j]\n",
    "#                     idx = proc_indices[i, j]\n",
    "#                     proc_contri[i,idx] += con\n",
    "#             #print(\"contrib\",cond_contri.shape)        \n",
    "#             for i in range(med_pool.shape[0]):\n",
    "#                 for j in range(med_pool.shape[1]):\n",
    "#                     con = med_pool[i,j] * med_fc_weight[j]\n",
    "#                     idx = med_indices[i, j]\n",
    "#                     med_contri[i,idx] += con\n",
    "#             #print(\"contrib\",cond_contri.shape)        \n",
    "#             for i in range(lab_pool.shape[0]):\n",
    "#                 for j in range(lab_pool.shape[1]):\n",
    "#                     con = lab_pool[i,j] * lab_fc_weight[j]\n",
    "#                     idx = lab_indices[i, j]\n",
    "#                     lab_contri[i,idx] += con\n",
    "#             #print(\"contrib\",cond_contri.shape)\n",
    "#             for i in range(census_pool.shape[0]):\n",
    "#                 for j in range(census_pool.shape[1]):\n",
    "#                     con = census_pool[i,j] * census_fc_weight[j]\n",
    "#                     idx = census_indices[i, j]\n",
    "#                     census_contri[i,idx] += con\n",
    "#             #print(\"contrib\",cond_contri[0])\n",
    "#             cond_output = cond_output.data.cpu().numpy()\n",
    "#             return sigout1,out1, cond_contri,proc_contri,med_contri,lab_contri,census_contri,cond_pool,cond_fc_weight,cond_indices,conds\n",
    "#         else:\n",
    "#             return sigout1\n",
    "            \n",
    "        \n",
    "#     def init_hidden(self):\n",
    "#         # initialize the hidden state and the cell state to zeros\n",
    "#         h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "#         c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "# #         if self.hparams.on_gpu:\n",
    "# #             hidden_a = hidden_a.cuda()\n",
    "# #             hidden_b = hidden_b.cuda()\n",
    "\n",
    "#         h = Variable(h)\n",
    "#         c = Variable(c)\n",
    "\n",
    "#         return (h, c)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNStatic(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(CNNStatic, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=CodeCNN(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.med=CodeCNN(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.bmi=CodeCNN(self.device,self.embed_size,self.rnn_size,None,self.bmi_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=True)\n",
    "        \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.bmi_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        \n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        \n",
    "        cond_pool,cond_indices =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond\")\n",
    "        #proc_pool,proc_indices = self.proc(procs, proc_lengths)      \n",
    "        med_pool,med_indices = self.med(meds, med_lengths)    \n",
    "        #print(\"med\")\n",
    "        #lab_pool,lab_indices = self.lab(labs, lab_lengths)  \n",
    "        bmi_pool,bmi_indices =self.bmi(bmi, bmi_lengths)\n",
    "        #print(\"bmi\")\n",
    "        \n",
    "\n",
    "        #print(\"hi\")\n",
    "        cond_out1 = self.cond_fc(cond_pool)\n",
    "        #proc_out1 = self.proc_fc(proc_pool)\n",
    "        med_out1 = self.med_fc(med_pool)\n",
    "        #lab_out1 = self.lab_fc(lab_pool)\n",
    "        bmi_out1 = self.bmi_fc(bmi_pool)\n",
    "        \n",
    "        \n",
    "        #print(cond_out1.shape)\n",
    "        #print(proc_out1.shape)\n",
    "        #print(med_out1.shape)\n",
    "        #print(lab_out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        #out1=torch.cat((cond_out1,proc_out1,med_out1,lab_out1,census_out1),1)\n",
    "        #out1=torch.add(cond_out1, med_out1)\n",
    "        \n",
    "        #out1=torch.add(out1, lab_out1)\n",
    "        out1=torch.add(cond_out1, bmi_out1)\n",
    "        #print(\"hi\")\n",
    "        out1=torch.add(out1, med_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        \n",
    "        #print(\"hi\")\n",
    "        #print(out1)\n",
    "        #out1=self.fc(out1)\n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",out1.shape)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMax(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(CNNMax, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=CodeCNN(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.med=CodeCNN(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.bmi=CodeCNN(self.device,self.embed_size,self.rnn_size,None,self.bmi_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=True)\n",
    "        \n",
    "        self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)             \n",
    "        self.census_max = nn.AdaptiveMaxPool1d(1, True)        \n",
    "        \n",
    "        \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.bmi_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.census_fc=nn.Linear(self.embed_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        \n",
    "        cond_pool,cond_indices =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond\")\n",
    "        #proc_pool,proc_indices = self.proc(procs, proc_lengths)      \n",
    "        med_pool,med_indices = self.med(meds, med_lengths)    \n",
    "        #print(\"med\")\n",
    "        #lab_pool,lab_indices = self.lab(labs, lab_lengths)  \n",
    "        bmi_pool,bmi_indices =self.bmi(bmi, bmi_lengths)\n",
    "        #print(\"bmi\")\n",
    "        census=census.to(self.device)              \n",
    "        census=self.censusEmbed(census)     \n",
    "        census_output=census.view(census.shape[0],census.shape[2],census.shape[1])\n",
    "        census_output=F.relu(census_output)\n",
    "        census_pool, census_indices = self.census_max(census_output)               \n",
    "        census_pool,census_indices  = torch.squeeze(census_pool), torch.squeeze(census_indices)\n",
    "\n",
    "        #print(\"hi\")\n",
    "        cond_out1 = self.cond_fc(cond_pool)\n",
    "        #proc_out1 = self.proc_fc(proc_pool)\n",
    "        med_out1 = self.med_fc(med_pool)\n",
    "        #lab_out1 = self.lab_fc(lab_pool)\n",
    "        bmi_out1 = self.bmi_fc(bmi_pool)\n",
    "        census_out1 = self.census_fc(census_pool)\n",
    "        \n",
    "        #print(cond_out1.shape)\n",
    "        #print(proc_out1.shape)\n",
    "        #print(med_out1.shape)\n",
    "        #print(lab_out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        #out1=torch.cat((cond_out1,proc_out1,med_out1,lab_out1,census_out1),1)\n",
    "        #out1=torch.add(cond_out1, med_out1)\n",
    "        \n",
    "        #out1=torch.add(out1, lab_out1)\n",
    "        out1=torch.add(cond_out1, bmi_out1)\n",
    "        #print(\"hi\")\n",
    "        out1=torch.add(out1, med_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        out1=torch.add(out1, census_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1)\n",
    "        #out1=self.fc(out1)\n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",out1.shape)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        if contrib:\n",
    "            cond_pool = cond_pool.data.cpu().numpy()\n",
    "            #proc_pool = proc_pool.data.cpu().numpy()\n",
    "            #med_pool = med_pool.data.cpu().numpy()\n",
    "            #lab_pool = lab_pool.data.cpu().numpy()\n",
    "            bmi_pool = bmi_pool.data.cpu().numpy()\n",
    "            census_pool = census_pool.data.cpu().numpy()\n",
    "            #print(cond_pool.shape)\n",
    "            assert cond_pool.shape == cond_indices.shape\n",
    "            cond_fc_weight = self.cond_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #proc_fc_weight = self.proc_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #med_fc_weight = self.med_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #lab_fc_weight = self.lab_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            bmi_fc_weight = self.bmi_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            census_fc_weight = self.census_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #print(\"logit_weight\",fc_weight.shape)\n",
    "            cond_contri = np.zeros((self.batch_size, self.cond_seq_len))\n",
    "            #proc_contri = np.zeros((self.batch_size, self.proc_seq_len))\n",
    "            #med_contri = np.zeros((self.batch_size, self.med_seq_len))\n",
    "            #lab_contri = np.zeros((self.batch_size, self.lab_seq_len))\n",
    "            bmi_contri = np.zeros((self.batch_size, self.bmi_seq_len))\n",
    "            census_contri = np.zeros((self.batch_size, self.census_vocab_size))\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(cond_pool.shape[0]):\n",
    "                for j in range(cond_pool.shape[1]):\n",
    "                    con = cond_pool[i,j] * cond_fc_weight[j]\n",
    "                    idx = cond_indices[i, j]\n",
    "                    cond_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(proc_pool.shape[0]):\n",
    "#                for j in range(proc_pool.shape[1]):\n",
    "#                    con = proc_pool[i,j] * proc_fc_weight[j]\n",
    "#                    idx = proc_indices[i, j]\n",
    "#                    proc_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#             for i in range(med_pool.shape[0]):\n",
    "#                 for j in range(med_pool.shape[1]):\n",
    "#                     con = med_pool[i,j] * med_fc_weight[j]\n",
    "#                     idx = med_indices[i, j]\n",
    "#                     med_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(lab_pool.shape[0]):\n",
    "#                for j in range(lab_pool.shape[1]):\n",
    "#                    con = lab_pool[i,j] * lab_fc_weight[j]\n",
    "#                    idx = lab_indices[i, j]\n",
    "#                    lab_contri[i,idx] += con\n",
    "            for i in range(bmi_pool.shape[0]):\n",
    "                for j in range(bmi_pool.shape[1]):\n",
    "                    con = bmi_pool[i,j] * bmi_fc_weight[j]\n",
    "                    idx = bmi_indices[i, j]\n",
    "                    bmi_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(census_pool.shape[0]):\n",
    "                for j in range(census_pool.shape[1]):\n",
    "                    con = census_pool[i,j] * census_fc_weight[j]\n",
    "                    idx = census_indices[i, j]\n",
    "                    census_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri[0])\n",
    "            #cond_output = cond_output.data.cpu().numpy()\n",
    "            #return sigout1,out1, cond_contri,proc_contri,med_contri,lab_contri,census_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "            return sigout1,out1, cond_contri,bmi_contri,census_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "        else:\n",
    "            return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeCNN(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeCNN, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        if self.bmi_flag:\n",
    "            self.conv1 = nn.Conv1d(self.embed_size+1,self.rnn_size, kernel_size = 3, stride = 1, padding =0)\n",
    "        else:\n",
    "            self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "            self.conv1 = nn.Conv1d(self.embed_size * 2,self.rnn_size, kernel_size = 3, stride = 1, padding =0)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rnn_size)\n",
    "        self.maxpool1 = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.timeEmbed = nn.Embedding(self.time_vocab_size, self.embed_size). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(self.time_vocab_size, self.embed_size))\n",
    "\n",
    "    def forward(self, code,code_lengths):\n",
    "        #print(conds.shape)\n",
    "        code_time=code[1]\n",
    "        code=code[0]\n",
    "        \n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        #print(\"embedded\")\n",
    "        if self.bmi_flag:\n",
    "            code=code.unsqueeze(2)\n",
    "            #print(\"code\",code.shape)\n",
    "            #print(self.code_seq_len)\n",
    "            #code=code.type(torch.LongTensor)\n",
    "        else:\n",
    "            code=self.codeEmbed(code)\n",
    "        \n",
    "        #code_time=code_time.type(torch.LongTensor)\n",
    "        #h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "        code_time=self.timeEmbed(code_time)\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        code=code.type(torch.FloatTensor)\n",
    "        code_time=code_time.type(torch.FloatTensor)\n",
    "        code_time, code = code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        code=torch.cat((code,code_time),dim=2)\n",
    "        #code = self.dropout1(code)\n",
    "        code=code.permute(0,2,1)\n",
    "        #print(\"input\",code.shape)\n",
    "        #code=self.code_drop(code) \n",
    "        \n",
    "        #Run through LSTM\n",
    "        cond_output = self.conv1(code)\n",
    "        #print(\"output\",cond_output.shape)\n",
    "        cond_output = self.bn1(cond_output)\n",
    "        #print(\"output\",cond_output.shape)\n",
    "        cond_output = F.relu(cond_output)\n",
    "        #print(\"output\",cond_output.shape)\n",
    "\n",
    "        code_pool, code_indices = self.maxpool1(cond_output)\n",
    "        #print(\"output\",code_pool.shape)\n",
    "        \n",
    "        \n",
    "        code_pool = torch.squeeze(code_pool)\n",
    "        code_indices = torch.squeeze(code_indices)\n",
    "        \n",
    "        return code_pool,code_indices\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStatic(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMStatic, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=CodeBase(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.med=CodeBase(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.bmi=CodeBase(self.device,self.embed_size,self.rnn_size,None,self.bmi_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=True)\n",
    "        \n",
    "        #self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)                           \n",
    "        \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.bmi_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.census_fc=nn.Linear(self.census_vocab_size*self.embed_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        \n",
    "        cond_h_n =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond\")\n",
    "        #proc_pool,proc_indices = self.proc(procs, proc_lengths)      \n",
    "        med_h_n = self.med(meds, med_lengths)    \n",
    "        #print(\"med\")\n",
    "        #lab_pool,lab_indices = self.lab(labs, lab_lengths)  \n",
    "        bmi_h_n =self.bmi(bmi, bmi_lengths)\n",
    "        #print(\"bmi\")\n",
    "        \n",
    "\n",
    "        #print(\"hi\")\n",
    "        cond_out1 = self.cond_fc(cond_h_n)\n",
    "        #proc_out1 = self.proc_fc(proc_pool)\n",
    "        med_out1 = self.med_fc(med_h_n)\n",
    "        #lab_out1 = self.lab_fc(lab_pool)\n",
    "        bmi_out1 = self.bmi_fc(bmi_h_n)\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        #print(\"hi\")\n",
    "        #out1=torch.add(out1, lab_out1)\n",
    "        out1=torch.add(cond_out1, bmi_out1)\n",
    "        #print(\"hi\")\n",
    "        out1=torch.add(out1, med_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        #print(\"hi\")\n",
    "        #print(out1)\n",
    "        #out1=self.fc(out1)\n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",out1.shape)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBase(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMBase, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=CodeBase(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.med=CodeBase(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.bmi=CodeBase(self.device,self.embed_size,self.rnn_size,None,self.bmi_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=True)\n",
    "        \n",
    "        self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)                           \n",
    "        \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.bmi_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.census_fc=nn.Linear(self.census_vocab_size*self.embed_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        \n",
    "        cond_h_n =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond\")\n",
    "        #proc_pool,proc_indices = self.proc(procs, proc_lengths)      \n",
    "        med_h_n = self.med(meds, med_lengths)    \n",
    "        #print(\"med\")\n",
    "        #lab_pool,lab_indices = self.lab(labs, lab_lengths)  \n",
    "        bmi_h_n =self.bmi(bmi, bmi_lengths)\n",
    "        #print(\"bmi\")\n",
    "        census=census.to(self.device)              \n",
    "        census=self.censusEmbed(census)     \n",
    "        census_output=torch.reshape(census,(census.shape[0],-1))\n",
    "        #print(\"census\",census_output.shape)\n",
    "\n",
    "        #print(\"hi\")\n",
    "        cond_out1 = self.cond_fc(cond_h_n)\n",
    "        #proc_out1 = self.proc_fc(proc_pool)\n",
    "        med_out1 = self.med_fc(med_h_n)\n",
    "        #lab_out1 = self.lab_fc(lab_pool)\n",
    "        bmi_out1 = self.bmi_fc(bmi_h_n)\n",
    "        #print(\"hi\")\n",
    "        census_out1 = self.census_fc(census_output)\n",
    "        #print(\"hi\")\n",
    "        #out1=torch.add(out1, lab_out1)\n",
    "        out1=torch.add(cond_out1, bmi_out1)\n",
    "        #print(\"hi\")\n",
    "        out1=torch.add(out1, med_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        out1=torch.add(out1, census_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1)\n",
    "        #out1=self.fc(out1)\n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",out1.shape)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeBase(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeBase, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        if self.bmi_flag:\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size + 1,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        else:\n",
    "            self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size*2,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "            \n",
    "        self.code_drop = nn.Dropout(0.2)\n",
    "        \n",
    "        self.timeEmbed = nn.Embedding(self.time_vocab_size, self.embed_size). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(self.time_vocab_size, self.embed_size))\n",
    "\n",
    "    def forward(self, code,code_lengths):\n",
    "        #print(conds.shape)\n",
    "        code_time=code[1]\n",
    "        code=code[0]\n",
    "        \n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        #print(\"embedded\")\n",
    "        if self.bmi_flag:\n",
    "            code=code.unsqueeze(2)\n",
    "            #print(\"code\",code.shape)\n",
    "            #print(self.code_seq_len)\n",
    "            #code=code.type(torch.LongTensor)\n",
    "        else:\n",
    "            code=self.codeEmbed(code)\n",
    "        \n",
    "        #code_time=code_time.type(torch.LongTensor)\n",
    "        #h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "        code_time=self.timeEmbed(code_time)\n",
    "        #print(\"hi\")\n",
    "        #print(code_time.shape)\n",
    "        code=code.type(torch.FloatTensor)\n",
    "        code_time=code_time.type(torch.FloatTensor)\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        code=torch.cat((code,code_time),dim=2)\n",
    "        #print(\"hi\")\n",
    "        #code=self.code_drop(code) \n",
    "        #Pack all sequences\n",
    "        code_pack = torch.nn.utils.rnn.pack_padded_sequence(code, code_lengths, batch_first=True,enforce_sorted=False)\n",
    "        #print(\"hi\")\n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code_pack, (h_0, c_0))\n",
    "        #print(\"hi\")\n",
    "        #print(\"output\",code_h_n.shape)\n",
    "        code_h_n=code_h_n.squeeze()\n",
    "                      \n",
    "        #print(\"output\",code_h_n.shape)\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        return code_h_n\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttn(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMAttn, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=CodeAttn(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.med=CodeAttn(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.bmi=CodeAttn(self.device,self.embed_size,self.rnn_size,None,self.bmi_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=True)\n",
    "        \n",
    "        self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)             \n",
    "        self.census_max = nn.AdaptiveMaxPool1d(1, True)        \n",
    "        \n",
    "        \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.bmi_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.census_fc=nn.Linear(self.census_vocab_size*self.embed_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        \n",
    "        cond_context =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond\")\n",
    "        #proc_pool,proc_indices = self.proc(procs, proc_lengths)      \n",
    "        med_context = self.med(meds, med_lengths)    \n",
    "        #print(\"med\")\n",
    "        #lab_pool,lab_indices = self.lab(labs, lab_lengths)  \n",
    "        bmi_context =self.bmi(bmi, bmi_lengths)\n",
    "        #print(\"bmi\")\n",
    "        census=census.to(self.device)              \n",
    "        census=self.censusEmbed(census)     \n",
    "        census_output=torch.reshape(census,(census.shape[0],-1))\n",
    "\n",
    "        #print(\"hi\")\n",
    "        cond_out1 = self.cond_fc(cond_context)\n",
    "        #print(\"hi\")\n",
    "        #proc_out1 = self.proc_fc(proc_pool)\n",
    "        med_out1 = self.med_fc(med_context)\n",
    "        #print(\"hi\")\n",
    "        #lab_out1 = self.lab_fc(lab_pool)\n",
    "        bmi_out1 = self.bmi_fc(bmi_context)\n",
    "        #print(\"hi\")\n",
    "        census_out1 = self.census_fc(census_output)\n",
    "        \n",
    "        #print(cond_out1.shape)\n",
    "        #print(proc_out1.shape)\n",
    "        #print(med_out1.shape)\n",
    "        #print(bmi_out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        #out1=torch.cat((cond_out1,proc_out1,med_out1,lab_out1,census_out1),1)\n",
    "        #out1=torch.add(cond_out1, med_out1)\n",
    "        \n",
    "        #out1=torch.add(out1, lab_out1)\n",
    "        out1=torch.add(cond_out1, bmi_out1)\n",
    "        #print(\"hi\")\n",
    "        out1=torch.add(out1, med_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        out1=torch.add(out1, census_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1)\n",
    "        #out1=self.fc(out1)\n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",out1.shape)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAttn(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeAttn, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        if self.bmi_flag:\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size + 1,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        else:\n",
    "            self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size*2,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "            \n",
    "        self.code_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.code_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.code_drop = nn.Dropout(0.2)\n",
    "        \n",
    "        self.timeEmbed = nn.Embedding(self.time_vocab_size, self.embed_size). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(self.time_vocab_size, self.embed_size))\n",
    "\n",
    "    def forward(self, code,code_lengths):\n",
    "        #print(conds.shape)\n",
    "        code_time=code[1]\n",
    "        code=code[0]\n",
    "        \n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        #print(\"embedded\")\n",
    "        if self.bmi_flag:\n",
    "            code=code.unsqueeze(2)\n",
    "            #print(\"code\",code.shape)\n",
    "            #print(self.code_seq_len)\n",
    "            #code=code.type(torch.LongTensor)\n",
    "        else:\n",
    "            code=self.codeEmbed(code)\n",
    "        \n",
    "        #code_time=code_time.type(torch.LongTensor)\n",
    "        #h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "        code_time=self.timeEmbed(code_time)\n",
    "        #print(\"hi\")\n",
    "        #print(code_time.shape)\n",
    "        code=code.type(torch.FloatTensor)\n",
    "        code_time=code_time.type(torch.FloatTensor)\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        code=torch.cat((code,code_time),dim=2)\n",
    "        #print(\"hi\")\n",
    "        code=self.code_drop(code) \n",
    "        #Pack all sequences\n",
    "        code_pack = torch.nn.utils.rnn.pack_padded_sequence(code, code_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code_pack, (h_0, c_0))\n",
    "       \n",
    "        #unpack sequence\n",
    "        code_output, _ = torch.nn.utils.rnn.pad_packed_sequence(code_output, batch_first=True)\n",
    "        #print(\"output\",code_output.shape)\n",
    "        code_softmax=self.code_fc(code_output)\n",
    "        #print(\"softmax\",code_softmax.shape)\n",
    "        code_softmax=F.softmax(code_softmax)\n",
    "        #print(\"softmax\",code_softmax.shape)\n",
    "        code_softmax=torch.sum(torch.mul(code_output,code_softmax),dim=1)\n",
    "        #print(\"softmax\",code_softmax.shape)\n",
    "        #print(\"========================\")\n",
    "               \n",
    "        #code_output=F.relu(code_output)\n",
    "                      \n",
    "        \n",
    "        \n",
    "        return code_softmax\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMMaxStatic(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMMaxStatic, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.med=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        self.bmi=CodeLSTM(self.device,self.embed_size,self.rnn_size,None,self.bmi_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=True)\n",
    "        \n",
    "        \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.bmi_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        \n",
    "        cond_pool,cond_indices =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond\")\n",
    "        #proc_pool,proc_indices = self.proc(procs, proc_lengths)      \n",
    "        med_pool,med_indices = self.med(meds, med_lengths)    \n",
    "        #print(\"med\")\n",
    "        #lab_pool,lab_indices = self.lab(labs, lab_lengths)  \n",
    "        bmi_pool,bmi_indices =self.bmi(bmi, bmi_lengths)\n",
    "        #print(\"bmi\")\n",
    "        \n",
    "        #print(\"hi\")\n",
    "        cond_out1 = self.cond_fc(cond_pool)\n",
    "        #proc_out1 = self.proc_fc(proc_pool)\n",
    "        med_out1 = self.med_fc(med_pool)\n",
    "        #lab_out1 = self.lab_fc(lab_pool)\n",
    "        bmi_out1 = self.bmi_fc(bmi_pool)\n",
    "       \n",
    "        #print(cond_out1.shape)\n",
    "        #print(proc_out1.shape)\n",
    "        #print(med_out1.shape)\n",
    "        #print(lab_out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        #out1=torch.cat((cond_out1,proc_out1,med_out1,lab_out1,census_out1),1)\n",
    "        #out1=torch.add(cond_out1, med_out1)\n",
    "        \n",
    "        #out1=torch.add(out1, lab_out1)\n",
    "        out1=torch.add(cond_out1, bmi_out1)\n",
    "        #print(\"hi\")\n",
    "        out1=torch.add(out1, med_out1)\n",
    "        #print(\"hi\")\n",
    "        #print(out1.shape)\n",
    "        #print(census_out1.shape)\n",
    "        \n",
    "        #print(out1)\n",
    "        #out1=self.fc(out1)\n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",out1.shape)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        if contrib:\n",
    "            cond_pool = cond_pool.data.cpu().numpy()\n",
    "            #proc_pool = proc_pool.data.cpu().numpy()\n",
    "            med_pool = med_pool.data.cpu().numpy()\n",
    "            #lab_pool = lab_pool.data.cpu().numpy()\n",
    "            bmi_pool = bmi_pool.data.cpu().numpy()\n",
    "            \n",
    "            assert cond_pool.shape == cond_indices.shape\n",
    "            cond_fc_weight = self.cond_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #proc_fc_weight = self.proc_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            med_fc_weight = self.med_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #lab_fc_weight = self.lab_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            bmi_fc_weight = self.bmi_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            census_fc_weight = self.census_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #print(\"logit_weight\",fc_weight.shape)\n",
    "            cond_contri = np.zeros((self.batch_size, self.cond_seq_len))\n",
    "            #proc_contri = np.zeros((self.batch_size, self.proc_seq_len))\n",
    "            med_contri = np.zeros((self.batch_size, self.med_seq_len))\n",
    "            #lab_contri = np.zeros((self.batch_size, self.lab_seq_len))\n",
    "            bmi_contri = np.zeros((self.batch_size, self.bmi_seq_len))\n",
    "            \n",
    "            for i in range(cond_pool.shape[0]):\n",
    "                for j in range(cond_pool.shape[1]):\n",
    "                    con = cond_pool[i,j] * cond_fc_weight[j]\n",
    "                    idx = cond_indices[i, j]\n",
    "                    cond_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(proc_pool.shape[0]):\n",
    "#                for j in range(proc_pool.shape[1]):\n",
    "#                    con = proc_pool[i,j] * proc_fc_weight[j]\n",
    "#                    idx = proc_indices[i, j]\n",
    "#                    proc_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "            for i in range(med_pool.shape[0]):\n",
    "                for j in range(med_pool.shape[1]):\n",
    "                    con = med_pool[i,j] * med_fc_weight[j]\n",
    "                    idx = med_indices[i, j]\n",
    "                    med_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(lab_pool.shape[0]):\n",
    "#                for j in range(lab_pool.shape[1]):\n",
    "#                    con = lab_pool[i,j] * lab_fc_weight[j]\n",
    "#                    idx = lab_indices[i, j]\n",
    "#                    lab_contri[i,idx] += con\n",
    "            for i in range(bmi_pool.shape[0]):\n",
    "                for j in range(bmi_pool.shape[1]):\n",
    "                    con = bmi_pool[i,j] * bmi_fc_weight[j]\n",
    "                    idx = bmi_indices[i, j]\n",
    "                    bmi_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            \n",
    "            #print(\"contrib\",cond_contri[0])\n",
    "            #cond_output = cond_output.data.cpu().numpy()\n",
    "            #return sigout1,out1, cond_contri,proc_contri,med_contri,lab_contri,census_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "            return sigout1,out1, cond_contri,bmi_contri,med_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "        else:\n",
    "            return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMMax(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_vocab_size,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMMax, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_vocab_size=bmi_vocab_size\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=1\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        self.proc=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        self.med=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        #self.bmi=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.bmi_vocab_size,self.bmi_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=True)\n",
    "        \n",
    "        #self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)             \n",
    "        #self.census_max = nn.AdaptiveMaxPool1d(1, True)        \n",
    "        \n",
    "        self.fc=nn.Linear(3*self.rnn_size, 1, False)\n",
    "        #self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.proc_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.med_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.lab_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.bmi_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        #self.census_fc=nn.Linear(self.embed_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,contrib):        \n",
    "        \n",
    "        cond_pool,cond_indices =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond_pool\",cond_pool.shape)\n",
    "        #cond_pool,cond_indices,cond_pool_ob,cond_indices_ob =self.cond(conds, cond_lengths)\n",
    "        #print(\"cond\")\n",
    "        proc_pool,proc_indices = self.proc(procs, proc_lengths)   \n",
    "        #print(\"cond_pool\",proc_pool.shape)\n",
    "        med_pool,med_indices = self.med(meds, med_lengths)  \n",
    "        #print(\"cond_pool\",med_pool.shape)\n",
    "        #med_pool,med_indices,med_pool_ob,med_indices_ob =self.med(meds, med_lengths)\n",
    "        #print(\"med\")\n",
    "        #lab_pool,lab_indices = self.lab(labs, lab_lengths)  \n",
    "        #bmi_pool,bmi_indices =self.bmi(bmi, bmi_lengths)\n",
    "        #print(\"bmi\")\n",
    "#         census=census.to(self.device)              \n",
    "#         census=self.censusEmbed(census)     \n",
    "#         census_output=census.view(census.shape[0],census.shape[2],census.shape[1])\n",
    "#         census_output=F.relu(census_output)\n",
    "#         census_pool, census_indices = self.census_max(census_output)               \n",
    "#         census_pool,census_indices  = torch.squeeze(census_pool), torch.squeeze(census_indices)\n",
    "\n",
    "        #print(\"hi\")\n",
    "        #cond_out1 = self.cond_fc(cond_pool)\n",
    "        #proc_out1 = self.proc_fc(proc_pool)\n",
    "        #med_out1 = self.med_fc(med_pool)\n",
    "        #lab_out1 = self.lab_fc(lab_pool)\n",
    "        #bmi_out1 = self.bmi_fc(bmi_pool)\n",
    "        #census_out1 = self.census_fc(census_pool)\n",
    "        \n",
    "        \n",
    "        #print(\"cond_pool_ob\",cond_pool_ob.shape)\n",
    "        #out1=torch.cat((cond_pool,cond_pool_ob),1)\n",
    "        #out1=cond_pool\n",
    "        out1=torch.cat((cond_pool,med_pool),1)\n",
    "        out1=torch.cat((out1,proc_pool),1)\n",
    "        out1 = self.fc(out1)\n",
    "        #print(\"out1\",out1.shape)\n",
    "        #print(\"out1\",out1)\n",
    "        #print(\"out1\",out1[16])\n",
    "        #out1=torch.add(cond_out1, med_out1)\n",
    "        fc_w=self.fc.weight.data\n",
    "        fc_w=torch.squeeze(fc_w)\n",
    "#         print(\"fc_w\",fc_w)\n",
    "        \n",
    "#         print(\"Next code_out\",cond_pool[10])\n",
    "#         print(\"Next code_out_ob\",cond_pool_ob[10])\n",
    "        \n",
    "        #code_out=torch.mul(cond_pool,fc_w[0:cond_pool.shape[1]])\n",
    "        #code_out_ob=torch.mul(cond_pool_ob,fc_w[cond_pool.shape[1]:])\n",
    "        #med_out=torch.mul(med_pool,fc_w[cond_pool.shape[1]:])\n",
    "        \n",
    "        #print(\"Next code_out\",cond_pool[16])\n",
    "        #print(\"Cond Indices\",cond_indices[16])\n",
    "##         print(\"Next code_out_ob\",code_out_ob[10])\n",
    "        #print(\"Next med_out\",med_pool[16])\n",
    "        #print(\"Med Indices\",med_indices[16])\n",
    "        #print(\"Next code_out\",code_out[16].sum())\n",
    "##         print(\"Next code_out_ob\",code_out_ob[10].sum())\n",
    "        #print(\"Next med_out\",med_out[16].sum())\n",
    "##         print(code_out[0,:])\n",
    "        \n",
    "        \n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",sigout1[16])\n",
    "        #print(\"sig out\",sigout1)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        if contrib:\n",
    "            cond_pool = cond_pool.data.cpu().numpy()\n",
    "            #proc_pool = proc_pool.data.cpu().numpy()\n",
    "            med_pool = med_pool.data.cpu().numpy()\n",
    "            #lab_pool = lab_pool.data.cpu().numpy()\n",
    "            bmi_pool = bmi_pool.data.cpu().numpy()\n",
    "            census_pool = census_pool.data.cpu().numpy()\n",
    "            #print(cond_pool.shape)\n",
    "            assert cond_pool.shape == cond_indices.shape\n",
    "            cond_fc_weight = self.cond_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #proc_fc_weight = self.proc_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            med_fc_weight = self.med_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #lab_fc_weight = self.lab_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            bmi_fc_weight = self.bmi_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            census_fc_weight = self.census_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #print(\"logit_weight\",fc_weight.shape)\n",
    "            cond_contri = np.zeros((self.batch_size, self.cond_seq_len))\n",
    "            #proc_contri = np.zeros((self.batch_size, self.proc_seq_len))\n",
    "            med_contri = np.zeros((self.batch_size, self.med_seq_len))\n",
    "            #lab_contri = np.zeros((self.batch_size, self.lab_seq_len))\n",
    "            bmi_contri = np.zeros((self.batch_size, self.bmi_seq_len))\n",
    "            census_contri = np.zeros((self.batch_size, self.census_vocab_size))\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(cond_pool.shape[0]):\n",
    "                for j in range(cond_pool.shape[1]):\n",
    "                    con = cond_pool[i,j] * cond_fc_weight[j]\n",
    "                    idx = cond_indices[i, j]\n",
    "                    cond_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(proc_pool.shape[0]):\n",
    "#                for j in range(proc_pool.shape[1]):\n",
    "#                    con = proc_pool[i,j] * proc_fc_weight[j]\n",
    "#                    idx = proc_indices[i, j]\n",
    "#                    proc_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "            for i in range(med_pool.shape[0]):\n",
    "                for j in range(med_pool.shape[1]):\n",
    "                    con = med_pool[i,j] * med_fc_weight[j]\n",
    "                    idx = med_indices[i, j]\n",
    "                    med_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(lab_pool.shape[0]):\n",
    "#                for j in range(lab_pool.shape[1]):\n",
    "#                    con = lab_pool[i,j] * lab_fc_weight[j]\n",
    "#                    idx = lab_indices[i, j]\n",
    "#                    lab_contri[i,idx] += con\n",
    "            for i in range(bmi_pool.shape[0]):\n",
    "                for j in range(bmi_pool.shape[1]):\n",
    "                    con = bmi_pool[i,j] * bmi_fc_weight[j]\n",
    "                    idx = bmi_indices[i, j]\n",
    "                    bmi_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(census_pool.shape[0]):\n",
    "                for j in range(census_pool.shape[1]):\n",
    "                    con = census_pool[i,j] * census_fc_weight[j]\n",
    "                    idx = census_indices[i, j]\n",
    "                    census_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri[0])\n",
    "            #cond_output = cond_output.data.cpu().numpy()\n",
    "            #return sigout1,out1, cond_contri,proc_contri,med_contri,lab_contri,census_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "            return sigout1,out1, cond_contri,bmi_contri,census_contri,med_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "        else:\n",
    "            return sigout1,out1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "breeding-digit",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c7df64056b9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCodeLSTMAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcode_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcode_seq_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbmi_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodeLSTMAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CodeLSTMAll(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeLSTMAll, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "#         if self.bmi_flag:\n",
    "#             self.codeRnn = nn.LSTM(input_size=self.embed_size,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "#         else:\n",
    "        self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "        self.codeRnn = nn.LSTM(input_size=self.embed_size,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "\n",
    "        \n",
    "#        self.timeEmbed = nn.Embedding(self.time_vocab_size, self.embed_size). \\\n",
    "#            from_pretrained(embeddings=self._init_posi_embedding(self.time_vocab_size, self.embed_size))\n",
    "        \n",
    "        self.code_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "       \n",
    "        self.code_drop = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, code,code_lengths):\n",
    "        #print(conds.shape)\n",
    "        #ob=code[2]\n",
    "#        code_time=code[1]\n",
    "#        code=code[0]\n",
    "        #print()\n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        #print(code.shape)\n",
    "        #print(\"embedded\")\n",
    "#         if self.bmi_flag:\n",
    "#             code=code.unsqueeze(2)\n",
    "#             #print(\"code\",code.shape)\n",
    "#             #print(self.code_seq_len)\n",
    "#             #code=code.type(torch.LongTensor)\n",
    "#         else:\n",
    "        code=self.codeEmbed(code)\n",
    "        \n",
    "#        code_time=code_time.type(torch.LongTensor)\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "#        code_time=self.timeEmbed(code_time)\n",
    "        #print(\"hi\")\n",
    "        #print(code.shape)\n",
    "        code=code.type(torch.FloatTensor)\n",
    "#        code_time=code_time.type(torch.FloatTensor)\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "#        code=torch.cat((code,code_time),dim=2)\n",
    "        #print(\"hi\")\n",
    "        #code=self.code_drop(code) \n",
    "        #Pack all sequences\n",
    "        code_pack = torch.nn.utils.rnn.pack_padded_sequence(code, code_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code_pack, (h_0, c_0))\n",
    "       \n",
    "        #unpack sequence\n",
    "        code_output, _ = torch.nn.utils.rnn.pad_packed_sequence(code_output, batch_first=True)\n",
    "        \n",
    "               \n",
    "        \n",
    "        #code_output=F.relu(code_output)\n",
    "                      \n",
    "        #print(\"output\",code_output.shape)\n",
    "        #print(\"hi\")\n",
    "        code_output=code_output.view(code_output.shape[0],code_output.shape[2],code_output.shape[1])\n",
    "        \n",
    "        #code_output=F.relu(code_output)\n",
    "#         print(\"output relu\",code_output[99])\n",
    "        #print(\"======================\")\n",
    "        #print(code_output.shape)\n",
    "        #print(code_lengths.shape)\n",
    "        #print(\"======================\")\n",
    "        \n",
    "        code_pool=[]\n",
    "        code_indices=[]\n",
    "        \n",
    "        for i in range(code_output.shape[0]):\n",
    "            pool, indices = self.code_max(code_output[i:i+1,:,0:code_lengths[i]])\n",
    "            if i==0:\n",
    "                code_pool=pool\n",
    "                code_indices=indices\n",
    "            else:\n",
    "                code_pool=torch.cat((code_pool,pool),0)\n",
    "                code_indices=torch.cat((code_indices,indices),0)\n",
    "        #print(\"======================\")\n",
    "        ##print(\"code_indices\",code_indices.shape)\n",
    "        #print(\"======================\")\n",
    "        \n",
    " \n",
    "        code_pool = torch.squeeze(code_pool)\n",
    "        code_indices = torch.squeeze(code_indices)\n",
    "        #print(\"code_indices\",code_indices.shape)\n",
    "        \n",
    "        \n",
    "        return code_pool,code_indices\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeLSTMob(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeLSTMob, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        if self.bmi_flag:\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size + 1,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        else:\n",
    "            self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "            self.codeRnn = nn.LSTM(input_size=2*self.embed_size,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "            \n",
    "        \n",
    "        self.timeEmbed = nn.Embedding(self.time_vocab_size, self.embed_size). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(self.time_vocab_size, self.embed_size))\n",
    "        \n",
    "        self.code_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.code_max_ob = nn.AdaptiveMaxPool1d(1, True)\n",
    "       \n",
    "        self.code_drop = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, code,code_lengths):\n",
    "        #print(conds.shape)\n",
    "        ob=code[2]\n",
    "        code_time=code[1]\n",
    "        code=code[0]\n",
    "        \n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        #print(\"embedded\")\n",
    "        if self.bmi_flag:\n",
    "            code=code.unsqueeze(2)\n",
    "            #print(\"code\",code.shape)\n",
    "            #print(self.code_seq_len)\n",
    "            #code=code.type(torch.LongTensor)\n",
    "        else:\n",
    "            code=self.codeEmbed(code)\n",
    "        \n",
    "        code_time=code_time.type(torch.LongTensor)\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "        code_time=self.timeEmbed(code_time)\n",
    "        #print(\"hi\")\n",
    "#         print(code.shape)\n",
    "        code=code.type(torch.FloatTensor)\n",
    "        code_time=code_time.type(torch.FloatTensor)\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        code=torch.cat((code,code_time),dim=2)\n",
    "        #print(\"hi\")\n",
    "        #code=self.code_drop(code) \n",
    "        #Pack all sequences\n",
    "        code_pack = torch.nn.utils.rnn.pack_padded_sequence(code, code_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code_pack, (h_0, c_0))\n",
    "       \n",
    "        #unpack sequence\n",
    "        code_output, _ = torch.nn.utils.rnn.pad_packed_sequence(code_output, batch_first=True)\n",
    "        \n",
    "               \n",
    "        \n",
    "        #code_output=F.relu(code_output)\n",
    "                      \n",
    "#         print(\"output\",code_output.shape)\n",
    "        #print(\"hi\")\n",
    "        code_output=code_output.view(code_output.shape[0],code_output.shape[2],code_output.shape[1])\n",
    "        \n",
    "        code_output=F.relu(code_output)\n",
    "#         print(\"output relu\",code_output[99])     \n",
    "#         print(code_output.shape)\n",
    "        ob=ob.unsqueeze(1).repeat(1,self.rnn_size,1)\n",
    "#         print(\"ob\",ob.shape)\n",
    "        ob=ob[:,:,0:code_output.shape[2]]\n",
    "#         print(\"ob\",ob.shape)\n",
    "        ob=ob.type(torch.FloatTensor)\n",
    "        #print(\"ob\",ob[2,:])\n",
    "        ob=ob.to(self.device)\n",
    "        #print((code_output)[2,:,:])\n",
    "        #print((code_output*ob)[2,:,:])\n",
    "        code_pool_ob, code_indices_ob = self.code_max_ob(code_output*ob)\n",
    "        ob=(~(ob.int().bool())).int()\n",
    "#         print(\"ob\",ob[2,:])\n",
    "        ob=ob.type(torch.FloatTensor)\n",
    "        ob=ob.to(self.device)\n",
    "        #print((code_output*ob)[2,:,:])\n",
    "        code_pool, code_indices = self.code_max(code_output*ob)\n",
    "        #print(\"code_pool\",code_pool.shape)\n",
    "        \n",
    " \n",
    "        code_pool = torch.squeeze(code_pool)\n",
    "        code_indices = torch.squeeze(code_indices)\n",
    "        code_pool_ob = torch.squeeze(code_pool_ob)\n",
    "        code_indices_ob = torch.squeeze(code_indices_ob)\n",
    "#         print(\"code_indices\",code_indices.shape)\n",
    "#         print(\"code_indices_ob\",code_indices_ob.shape)\n",
    "        #print(\"code_pool\",code_pool.shape)\n",
    "        #print(code_pool_ob[2,:])\n",
    "        #print(code_pool[2,:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        return code_pool,code_indices,code_pool_ob,code_indices_ob\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeLSTM(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeLSTM, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        if self.bmi_flag:\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size + 1,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        else:\n",
    "            self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size*2,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "            \n",
    "        self.code_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.code_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.code_drop = nn.Dropout(0.2)\n",
    "        \n",
    "        self.timeEmbed = nn.Embedding(self.time_vocab_size, self.embed_size). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(self.time_vocab_size, self.embed_size))\n",
    "\n",
    "    def forward(self, code,code_lengths):\n",
    "        #print(conds.shape)\n",
    "        code_time=code[1]\n",
    "        code=code[0]\n",
    "        \n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        #print(\"embedded\")\n",
    "        if self.bmi_flag:\n",
    "            code=code.unsqueeze(2)\n",
    "            #print(\"code\",code.shape)\n",
    "            #print(self.code_seq_len)\n",
    "            #code=code.type(torch.LongTensor)\n",
    "        else:\n",
    "            code=self.codeEmbed(code)\n",
    "        \n",
    "        #code_time=code_time.type(torch.LongTensor)\n",
    "        #h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "        code_time=self.timeEmbed(code_time)\n",
    "        #print(\"hi\")\n",
    "        #print(code_time.shape)\n",
    "        code=code.type(torch.FloatTensor)\n",
    "        code_time=code_time.type(torch.FloatTensor)\n",
    "        h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "\n",
    "        code=torch.cat((code,code_time),dim=2)\n",
    "        #print(\"hi\")\n",
    "        code=self.code_drop(code) \n",
    "        #Pack all sequences\n",
    "        code_pack = torch.nn.utils.rnn.pack_padded_sequence(code, code_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code_pack, (h_0, c_0))\n",
    "       \n",
    "        #unpack sequence\n",
    "        code_output, _ = torch.nn.utils.rnn.pad_packed_sequence(code_output, batch_first=True)\n",
    "        \n",
    "               \n",
    "        \n",
    "        #code_output=F.relu(code_output)\n",
    "                      \n",
    "        #print(\"output\",cond_output.shape)\n",
    "        #print(\"hi\")\n",
    "        code_output=code_output.view(code_output.shape[0],code_output.shape[2],code_output.shape[1])\n",
    "        \n",
    "        code_pool, code_indices = self.code_max(code_output)\n",
    "        #print(\"indices\",cond_indices.shape)\n",
    " \n",
    "        code_pool = torch.squeeze(code_pool)\n",
    "        code_indices = torch.squeeze(code_indices)\n",
    "        \n",
    "        return code_pool,code_indices\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMMaxNew(nn.Module):\n",
    "    def __init__(self,device,\n",
    "                 cond_vocab_size,cond_seq_len,proc_vocab_size,\n",
    "                 proc_seq_len,med_vocab_size,med_seq_len,\n",
    "                 lab_vocab_size,lab_seq_len,\n",
    "                 bmi_seq_len,time_vocab_size,\n",
    "                 census_vocab_size,\n",
    "                 embed_size,rnn_size,\n",
    "                 batch_size,filter_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMMaxNew, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.ndepth=2\n",
    "        self.nbreadth=4\n",
    "        self.filter_size=filter_size\n",
    "        #self.meds={}\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=nn.ModuleList()\n",
    "#         self.med=nn.ModuleList()\n",
    "        \n",
    "        #if self.cond_seq_len%self.filter_size>0:\n",
    "         ##   pad=int(self.filter_size-(self.cond_seq_len%self.filter_size))\n",
    "         #   self.cond_seq_len=self.cond_seq_len+pad\n",
    "        nfilter= int(self.cond_seq_len/self.filter_size)\n",
    "        self.cond.append(nn.ModuleList([\n",
    "                CodeLSTMNew(self.device,\n",
    "                          self.embed_size,self.rnn_size,\n",
    "                          self.cond_vocab_size,self.cond_seq_len,\n",
    "                          self.time_vocab_size,\n",
    "                          self.batch_size,bmi_flag=False) for k in range(nfilter)]))\n",
    "\n",
    "#         self.med.append(nn.ModuleList([\n",
    "#             CodeLSTMNew(self.device,\n",
    "#                       self.embed_size,self.rnn_size,\n",
    "#                       self.cond_vocab_size,self.cond_seq_len,\n",
    "#                       self.time_vocab_size,\n",
    "#                       self.batch_size,bmi_flag=False) for k in range(4)]))\n",
    "        nfilter= int(nfilter/self.filter_size)\n",
    "        while nfilter>=4:           \n",
    "            self.cond.append(nn.ModuleList([\n",
    "                CodeLSTMNew2(self.device,\n",
    "                          self.embed_size,self.rnn_size,\n",
    "                          self.cond_vocab_size,self.cond_seq_len,\n",
    "                          self.time_vocab_size,\n",
    "                          self.batch_size,bmi_flag=False) for k in range(nfilter)]))\n",
    "            nfilter= (nfilter/self.filter_size)\n",
    "        #height=len(self.cond)\n",
    "        #root=len(self.cond[height-1])\n",
    "        \n",
    "        self.cond.append(nn.ModuleList([\n",
    "                CodeLSTMNew2(self.device,\n",
    "                          self.embed_size,self.rnn_size,\n",
    "                          self.cond_vocab_size,self.cond_seq_len,\n",
    "                          self.time_vocab_size,\n",
    "                          self.batch_size,bmi_flag=False)]))\n",
    "#             self.med.append(nn.ModuleList([\n",
    "#                 CodeLSTMNew2(self.device,\n",
    "#                           self.embed_size,self.rnn_size,\n",
    "#                           self.cond_vocab_size,self.cond_seq_len,\n",
    "#                           self.time_vocab_size,\n",
    "#                           self.batch_size,bmi_flag=False) for k in range(j)]))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.shared_fc=nn.Linear(2*self.rnn_size, 1, False)\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, \n",
    "                lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        for i in range(len(self.cond)):\n",
    "            #print(i)\n",
    "            #print(cond_lengths)\n",
    "            new_code_lengths=np.zeros(self.batch_size)\n",
    "            for j in range(len(self.cond[i])):\n",
    "                #print(\"Hi\")\n",
    "                if(i==0):\n",
    "                    #print(\"in if\")\n",
    "                    #print(j)\n",
    "                    start=j*self.filter_size\n",
    "                    end=(j+1)*self.filter_size\n",
    "                    #print(start)\n",
    "                    #print(end)\n",
    "                    \n",
    "                    #print(self.cond[i][j])\n",
    "                    #print(conds[:,:,start:end].shape)\n",
    "                    #print(cond_lengths)\n",
    "                    cond_lengths_next=[x - self.filter_size for x in cond_lengths]\n",
    "                    #print(cond_lengths_next)\n",
    "                    cond_lengths_next=np.asarray(cond_lengths_next)\n",
    "                    cond_lengths_next[cond_lengths_next<0]=0\n",
    "                    cond_lengths_next=list(cond_lengths_next)\n",
    "                    cond_lengths_current=[a - b for a, b in zip(cond_lengths, cond_lengths_next)]\n",
    "                    cond_lengths_current=np.asarray(cond_lengths_current)\n",
    "                    cond_lengths_current[cond_lengths_current==0]=1\n",
    "                    cond_lengths_current=list(cond_lengths_current)\n",
    "                    #print(cond_lengths_current)\n",
    "                    cond_lengths=cond_lengths_next\n",
    "#                     if torch.max(conds[:,:,start:end]):\n",
    "#                    else:\n",
    "#                     pad=(len(self.cond[i])-j)*2\n",
    "#                     padding=torch.zeros(cond_max.shape[0],pad,cond_max.shape[2])\n",
    "#                     padding=padding.to(self.device)\n",
    "#                     cond_max=torch.cat((cond_max,padding),2)\n",
    "                    \n",
    "                    cond_pool,cond_indices,fc_cond, cond_out,cond_pool_ob,cond_indices_ob,fc_cond_ob, cond_out_ob,new_code_lengths = self.cond[i][j](conds[:,:,start:end],cond_lengths_current,new_code_lengths,j)\n",
    "                elif(i==len(self.cond)-1): \n",
    "                    cond_pool,cond_indices,fc_cond, cond_out,cond_pool_ob,cond_indices_ob,fc_cond_ob, cond_out_ob,new_code_lengths = self.cond[i][j](conds,cond_lengths_current,new_code_lengths,j)\n",
    "                else:\n",
    "                    #print(\"in i else\")\n",
    "                    #print(j)\n",
    "                    start=j*self.filter_size\n",
    "                    end=(j+1)*self.filter_size\n",
    "                    cond_pool,cond_indices,fc_cond, cond_out,cond_pool_ob,cond_indices_ob,fc_cond_ob, cond_out_ob,new_code_lengths = self.cond[i][j](conds[:,start:end,:],cond_lengths_current,new_code_lengths,j)\n",
    "                if(j==0):\n",
    "                    cond_out=torch.cat((cond_out,cond_out_ob),dim=1)\n",
    "                    #print(\"max\",cond_out.shape)\n",
    "                    cond_max=cond_out.unsqueeze(1)\n",
    "                    #cond_max_ob=cond_out_ob.unsqueeze(1)\n",
    "                    #print(cond_max.shape)\n",
    "                    #print(cond_max_ob.shape)\n",
    "                    #cond_max=torch.cat((cond_max, cond_max_ob), 1)\n",
    "                    #cond_max=cond_out\n",
    "                    #print(\"conds max\",cond_max.shape)\n",
    "                else:\n",
    "                    cond_out=torch.cat((cond_out,cond_out_ob),dim=1)\n",
    "                    cond_max=torch.cat((cond_max,cond_out.unsqueeze(1)),dim=1)\n",
    "                    #cond_max=torch.cat((cond_max, torch.cat((cond_out.unsqueeze(1),cond_out_ob.unsqueeze(1)),1)), 1)\n",
    "                    #print(\"conds max\",cond_max.shape)\n",
    "            conds=cond_max\n",
    "            code_lengths=new_code_lengths\n",
    "            print(\"code_lengths\",code_lengths[99])\n",
    "            #meds=med_max\n",
    "            #print(\"conds max\",conds.shape)\n",
    "        #print('outside for')\n",
    "        conds=conds.squeeze()\n",
    "        #print(conds.shape)\n",
    "        conds=torch.reshape(conds,(conds.shape[0],-1))\n",
    "        #meds=meds.squeeze()\n",
    "        #print(\"conds max\",conds.shape)\n",
    "        #allfeat=torch.cat((conds,meds),1)\n",
    "        #print(allfeat.shape)\n",
    "        fc_out=self.shared_fc(conds)\n",
    "        #print(\"fc_out\",fc_out)\n",
    "        sigout=self.sig(fc_out)\n",
    "        #print(sigout)\n",
    "        if contrib:\n",
    "            cond_pool = cond_pool.data.cpu().numpy()\n",
    "            #proc_pool = proc_pool.data.cpu().numpy()\n",
    "            med_pool = med_pool.data.cpu().numpy()\n",
    "            #lab_pool = lab_pool.data.cpu().numpy()\n",
    "            bmi_pool = bmi_pool.data.cpu().numpy()\n",
    "            census_pool = census_pool.data.cpu().numpy()\n",
    "            #print(cond_pool.shape)\n",
    "            assert cond_pool.shape == cond_indices.shape\n",
    "            cond_fc_weight = self.cond_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #proc_fc_weight = self.proc_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            med_fc_weight = self.med_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #lab_fc_weight = self.lab_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            bmi_fc_weight = self.bmi_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            census_fc_weight = self.census_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #print(\"logit_weight\",fc_weight.shape)\n",
    "            cond_contri = np.zeros((self.batch_size, self.cond_seq_len))\n",
    "            #proc_contri = np.zeros((self.batch_size, self.proc_seq_len))\n",
    "            med_contri = np.zeros((self.batch_size, self.med_seq_len))\n",
    "            #lab_contri = np.zeros((self.batch_size, self.lab_seq_len))\n",
    "            bmi_contri = np.zeros((self.batch_size, self.bmi_seq_len))\n",
    "            census_contri = np.zeros((self.batch_size, self.census_vocab_size))\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(cond_pool.shape[0]):\n",
    "                for j in range(cond_pool.shape[1]):\n",
    "                    con = cond_pool[i,j] * cond_fc_weight[j]\n",
    "                    idx = cond_indices[i, j]\n",
    "                    cond_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(proc_pool.shape[0]):\n",
    "#                for j in range(proc_pool.shape[1]):\n",
    "#                    con = proc_pool[i,j] * proc_fc_weight[j]\n",
    "#                    idx = proc_indices[i, j]\n",
    "#                    proc_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "            for i in range(med_pool.shape[0]):\n",
    "                for j in range(med_pool.shape[1]):\n",
    "                    con = med_pool[i,j] * med_fc_weight[j]\n",
    "                    idx = med_indices[i, j]\n",
    "                    med_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(lab_pool.shape[0]):\n",
    "#                for j in range(lab_pool.shape[1]):\n",
    "#                    con = lab_pool[i,j] * lab_fc_weight[j]\n",
    "#                    idx = lab_indices[i, j]\n",
    "#                    lab_contri[i,idx] += con\n",
    "            for i in range(bmi_pool.shape[0]):\n",
    "                for j in range(bmi_pool.shape[1]):\n",
    "                    con = bmi_pool[i,j] * bmi_fc_weight[j]\n",
    "                    idx = bmi_indices[i, j]\n",
    "                    bmi_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(census_pool.shape[0]):\n",
    "                for j in range(census_pool.shape[1]):\n",
    "                    con = census_pool[i,j] * census_fc_weight[j]\n",
    "                    idx = census_indices[i, j]\n",
    "                    census_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri[0])\n",
    "            #cond_output = cond_output.data.cpu().numpy()\n",
    "            #return sigout1,out1, cond_contri,proc_contri,med_contri,lab_contri,census_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "            return sigout,out1, cond_contri,bmi_contri,census_contri,med_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "        else:\n",
    "            return sigout\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeLSTMNew(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,\n",
    "                 time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeLSTMNew, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        if self.bmi_flag:\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size + 1,\n",
    "                                   hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        else:\n",
    "            self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "            self.codeRnn = nn.LSTM(input_size=self.embed_size,hidden_size=self.rnn_size,\n",
    "                                   num_layers = 1,batch_first=True)\n",
    "            \n",
    "        self.code_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.code_max_ob = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.code_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.code_drop = nn.Dropout(0.2)\n",
    "        \n",
    "        self.code_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.code_fc_ob=nn.Linear(self.rnn_size, 1, False)\n",
    "\n",
    "    def forward(self, code,code_lengths,new_code_lengths,j):\n",
    "        #print(\"forward of codelstmnew\")\n",
    "        ob=code[2]\n",
    "        code_time=code[1]\n",
    "        code=code[0]\n",
    "        \n",
    "        for j_len in range(code.shape[0]):\n",
    "            if(code[j_len].sum()==0):\n",
    "                new_code_lengths[j_len]=j\n",
    "        print(\"cond\",code[99])\n",
    "        #print(\"ob\",ob.shape)\n",
    "        #print(code.shape)\n",
    "        #print(len(code_lengths))\n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code_time, code,ob = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device),ob.to(self.device)\n",
    "        #print(\"code\",code[0:10,:])\n",
    "        #Embedd all sequences\n",
    "        #print(\"embedded\")\n",
    "        if self.bmi_flag:\n",
    "            code=code.unsqueeze(2)\n",
    "            #print(\"code\",code.shape)\n",
    "            #print(self.code_seq_len)\n",
    "            #code=code.type(torch.LongTensor)\n",
    "        else:\n",
    "            code=self.codeEmbed(code)\n",
    "        \n",
    "        #code_time=code_time.type(torch.LongTensor)\n",
    "        #h_0, c_0, code_time, code = h_0.to(self.device), c_0.to(self.device),code_time.to(self.device),code.to(self.device)\n",
    "        \n",
    "        #print(\"hi\")\n",
    "        #print(\"embed code\",code[99])\n",
    "        #print(code_lengths)\n",
    "        code=code.type(torch.FloatTensor)\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #code=self.code_drop(code) \n",
    "        #Pack all sequences\n",
    "        code_pack = torch.nn.utils.rnn.pack_padded_sequence(code, code_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code_pack, (h_0, c_0))\n",
    "        \n",
    "        #unpack sequence\n",
    "        code_output, _ = torch.nn.utils.rnn.pad_packed_sequence(code_output, batch_first=True)\n",
    "        #print(\"output rnn\",code_output[99])\n",
    "               \n",
    "        \n",
    "        code_output=F.relu(code_output)\n",
    "        print(\"output relu\",code_output[99])     \n",
    "        #print(\"output relu\",code_output[0:10,:,:])\n",
    "        #print(\"hi\")\n",
    "        code_output=code_output.view(code_output.shape[0],code_output.shape[2],code_output.shape[1])\n",
    "        #print(\"output reshaped\",code_output.shape)\n",
    "        self.filter_size=10\n",
    "        if(code_output.shape[2]<self.filter_size):\n",
    "            pad=self.filter_size-code_output.shape[2]\n",
    "            padding=torch.zeros(code_output.shape[0],code_output.shape[1],pad)\n",
    "            padding=padding.to(self.device)\n",
    "            code_output=torch.cat((code_output,padding),2)\n",
    "            #print(\"output reshaped\",code_output.shape)\n",
    "        #print(\"ob\",ob[2,:])\n",
    "        ob=ob.unsqueeze(1).repeat(1,self.rnn_size,1)\n",
    "        #print(\"ob\",ob.shape)\n",
    "        ob=ob.type(torch.FloatTensor)\n",
    "        #print(\"ob\",ob[2,:])\n",
    "        ob=ob.to(self.device)\n",
    "        #print((code_output)[2,:,:])\n",
    "        #print((code_output*ob)[2,:,:])\n",
    "        code_pool_ob, code_indices_ob = self.code_max_ob(code_output*ob)\n",
    "        ob=(~(ob.int().bool())).int()\n",
    "        #print(\"ob\",ob[2,:])\n",
    "        ob=ob.type(torch.FloatTensor)\n",
    "        ob=ob.to(self.device)\n",
    "        #print((code_output*ob)[2,:,:])\n",
    "        code_pool, code_indices = self.code_max(code_output*ob)\n",
    "        #print(\"code_pool\",code_pool.shape)\n",
    "        \n",
    " \n",
    "        code_pool = torch.squeeze(code_pool)\n",
    "        code_indices = torch.squeeze(code_indices)\n",
    "        code_pool_ob = torch.squeeze(code_pool_ob)\n",
    "        code_indices_ob = torch.squeeze(code_indices_ob)\n",
    "        print(\"code_indices\",code_indices[99])\n",
    "        print(\"code_indices_ob\",code_indices_ob[99])\n",
    "        #print(\"code_pool\",code_pool.shape)\n",
    "        #print(code_pool_ob[2,:])\n",
    "        #print(code_pool[2,:])\n",
    "        \n",
    "        code_out = self.code_fc(code_pool)\n",
    "        code_out_ob = self.code_fc_ob(code_pool_ob)\n",
    "        print(\"code_out\",code_out[99])\n",
    "        print(\"code_out_ob\",code_out_ob[99])\n",
    "        \n",
    "        fc_w=self.code_fc.weight.data\n",
    "        fc_w=torch.squeeze(fc_w)\n",
    "        \n",
    "        fc_w_ob=self.code_fc_ob.weight.data\n",
    "        fc_w_ob=torch.squeeze(fc_w_ob)\n",
    "        #print(\"fc_w\",fc_w.shape)\n",
    "        #print(fc_w)\n",
    "        \n",
    "        code_out=torch.mul(code_pool,self.code_fc.weight.data)\n",
    "        code_out_ob=torch.mul(code_pool_ob,self.code_fc_ob.weight.data)\n",
    "        print(\"Next code_out\",code_out[99])\n",
    "        print(\"Next code_out_ob\",code_out_ob[99])\n",
    "        print(\"Next code_out\",code_out[99].sum())\n",
    "        print(\"Next code_out_ob\",code_out_ob[99].sum())\n",
    "        print(code_out[0,:])\n",
    "        return code_pool,code_indices,fc_w, code_out,code_pool_ob,code_indices_ob,fc_w_ob,code_out_ob, new_code_lengths\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeLSTMNew2(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,\n",
    "                 code_vocab_size,code_seq_len,\n",
    "                 time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeLSTMNew2, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.codeRnn = nn.LSTM(input_size=2*self.rnn_size,hidden_size=self.rnn_size,\n",
    "                               num_layers = 1,batch_first=True)\n",
    "            \n",
    "        self.code_max = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.code_max_ob = nn.AdaptiveMaxPool1d(1, True)\n",
    "        self.code_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.code_drop = nn.Dropout(0.2)\n",
    "        \n",
    "        self.code_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        self.code_fc_ob=nn.Linear(self.rnn_size, 1, False)\n",
    "\n",
    "    def forward(self, code,code_lengths,new_code_lengths,j):        \n",
    "        #print(\"in seond layer\")\n",
    "        #initialize hidden and cell state\n",
    "        for j_len in range(code.shape[0]):\n",
    "            if(code[j_len].sum()==0):\n",
    "                new_code_lengths[j_len]=j\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0,  code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #print(code_time.shape)\n",
    "        code=code.type(torch.FloatTensor)\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Run through LSTM\n",
    "        #print(code.shape)\n",
    "        code_pack = torch.nn.utils.rnn.pack_padded_sequence(code, code_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code_pack, (h_0, c_0))\n",
    "        \n",
    "        #unpack sequence\n",
    "        code_output, _ = torch.nn.utils.rnn.pad_packed_sequence(code_output, batch_first=True)\n",
    "        #print(\"rnn\",code_output.shape)\n",
    "        code_output=torch.reshape(code_output,(code_output.shape[0],code_output.shape[2],\n",
    "                                               code_output.shape[1]))\n",
    "        #print(\"reshaped\",code_output.shape)\n",
    "        ob=torch.zeros(code_output.shape[0],code_output.shape[1],code_output.shape[2])\n",
    "        ob=ob.type(torch.FloatTensor)\n",
    "        ob=ob.to(self.device)\n",
    "        for k in range(code_output.shape[2]):\n",
    "            if k%2==1:\n",
    "                ob[:,:,k]=1\n",
    "        #print(\"ob\",ob[0,0,:])\n",
    "        code_pool_ob, code_indices_ob = self.code_max(code_output*ob)\n",
    "        \n",
    "        ob=(~(ob.int().bool())).int()\n",
    "        ob=ob.type(torch.FloatTensor)\n",
    "        ob=ob.to(self.device)\n",
    "        code_pool, code_indices = self.code_max(code_output*ob)\n",
    "        #print(\"indices\",cond_indices.shape)\n",
    " \n",
    "        code_pool = torch.squeeze(code_pool)\n",
    "        code_indices = torch.squeeze(code_indices)\n",
    "        code_pool_ob = torch.squeeze(code_pool_ob)\n",
    "        code_indices_ob = torch.squeeze(code_indices_ob)\n",
    "        #print(\"code_pool\",code_pool.shape)\n",
    "        #print(code_pool[0,:])\n",
    "        code_out = self.code_fc(code_pool)\n",
    "        code_out_ob = self.code_fc_ob(code_pool_ob)\n",
    "        #print(\"code_out\",code_out.shape)\n",
    "        \n",
    "        fc_w=self.code_fc.weight.data\n",
    "        fc_w=torch.squeeze(fc_w)\n",
    "        \n",
    "        fc_w_ob=self.code_fc_ob.weight.data\n",
    "        fc_w_ob=torch.squeeze(fc_w_ob)\n",
    "        #print(\"fc_w\",fc_w.shape)\n",
    "        #print(fc_w)\n",
    "        \n",
    "        code_out=torch.mul(code_pool,self.code_fc.weight.data)\n",
    "        code_out_ob=torch.mul(code_pool_ob,self.code_fc_ob.weight.data)\n",
    "        #print(\"code_out\",code_out.shape)\n",
    "        #print(code_out[0,:])\n",
    "        return code_pool,code_indices,fc_w, code_out,code_pool_ob,code_indices_ob,fc_w_ob, code_out_ob,new_code_lengths\n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    \n",
    "    def _init_posi_embedding(self, time_vocab_size, embed_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / embed_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((time_vocab_size, embed_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(0, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(time_vocab_size):\n",
    "            for idx in np.arange(1, embed_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMMaxFlat(nn.Module):\n",
    "    def __init__(self,device,\n",
    "                 cond_vocab_size,cond_seq_len,proc_vocab_size,\n",
    "                 proc_seq_len,med_vocab_size,med_seq_len,\n",
    "                 lab_vocab_size,lab_seq_len,\n",
    "                 bmi_seq_len,time_vocab_size,\n",
    "                 census_vocab_size,\n",
    "                 embed_size,rnn_size,\n",
    "                 batch_size,filter_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMMaxNew, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=5\n",
    "        self.device=device\n",
    "        self.ndepth=2\n",
    "        self.nbreadth=4\n",
    "        self.filter_size=filter_size\n",
    "        #self.meds={}\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.cond=nn.ModuleList()\n",
    "#         self.med=nn.ModuleList()\n",
    "        \n",
    "        #if self.cond_seq_len%self.filter_size>0:\n",
    "         ##   pad=int(self.filter_size-(self.cond_seq_len%self.filter_size))\n",
    "         #   self.cond_seq_len=self.cond_seq_len+pad\n",
    "        nfilter= int(self.cond_seq_len/self.filter_size)\n",
    "        self.cond.append(nn.ModuleList([\n",
    "                CodeLSTMNew(self.device,\n",
    "                          self.embed_size,self.rnn_size,\n",
    "                          self.cond_vocab_size,self.cond_seq_len,\n",
    "                          self.time_vocab_size,\n",
    "                          self.batch_size,bmi_flag=False) for k in range(nfilter)]))\n",
    "\n",
    "#         self.med.append(nn.ModuleList([\n",
    "#             CodeLSTMNew(self.device,\n",
    "#                       self.embed_size,self.rnn_size,\n",
    "#                       self.cond_vocab_size,self.cond_seq_len,\n",
    "#                       self.time_vocab_size,\n",
    "#                       self.batch_size,bmi_flag=False) for k in range(4)]))\n",
    "        nfilter= int(nfilter/self.filter_size)\n",
    "        while nfilter>=4:           \n",
    "            self.cond.append(nn.ModuleList([\n",
    "                CodeLSTMNew2(self.device,\n",
    "                          self.embed_size,self.rnn_size,\n",
    "                          self.cond_vocab_size,self.cond_seq_len,\n",
    "                          self.time_vocab_size,\n",
    "                          self.batch_size,bmi_flag=False) for k in range(nfilter)]))\n",
    "            nfilter= (nfilter/self.filter_size)\n",
    "        #height=len(self.cond)\n",
    "        #root=len(self.cond[height-1])\n",
    "        \n",
    "        self.cond.append(nn.ModuleList([\n",
    "                CodeLSTMNew2(self.device,\n",
    "                          self.embed_size,self.rnn_size,\n",
    "                          self.cond_vocab_size,self.cond_seq_len,\n",
    "                          self.time_vocab_size,\n",
    "                          self.batch_size,bmi_flag=False)]))\n",
    "#             self.med.append(nn.ModuleList([\n",
    "#                 CodeLSTMNew2(self.device,\n",
    "#                           self.embed_size,self.rnn_size,\n",
    "#                           self.cond_vocab_size,self.cond_seq_len,\n",
    "#                           self.time_vocab_size,\n",
    "#                           self.batch_size,bmi_flag=False) for k in range(j)]))\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.shared_fc=nn.Linear(2*self.rnn_size, 1, False)\n",
    "        \n",
    "    def forward(self, conds, cond_lengths,procs, proc_lengths,meds, med_lengths,labs, \n",
    "                lab_lengths,bmi, bmi_lengths,census,contrib):        \n",
    "        for i in range(len(self.cond)):\n",
    "            #print(i)\n",
    "            #print(cond_lengths)\n",
    "            new_code_lengths=np.zeros(self.batch_size)\n",
    "            for j in range(len(self.cond[i])):\n",
    "                #print(\"Hi\")\n",
    "                if(i==0):\n",
    "                    #print(\"in if\")\n",
    "                    #print(j)\n",
    "                    start=j*self.filter_size\n",
    "                    end=(j+1)*self.filter_size\n",
    "                    #print(start)\n",
    "                    #print(end)\n",
    "                    \n",
    "                    #print(self.cond[i][j])\n",
    "                    #print(conds[:,:,start:end].shape)\n",
    "                    #print(cond_lengths)\n",
    "                    cond_lengths_next=[x - self.filter_size for x in cond_lengths]\n",
    "                    #print(cond_lengths_next)\n",
    "                    cond_lengths_next=np.asarray(cond_lengths_next)\n",
    "                    cond_lengths_next[cond_lengths_next<0]=0\n",
    "                    cond_lengths_next=list(cond_lengths_next)\n",
    "                    cond_lengths_current=[a - b for a, b in zip(cond_lengths, cond_lengths_next)]\n",
    "                    cond_lengths_current=np.asarray(cond_lengths_current)\n",
    "                    cond_lengths_current[cond_lengths_current==0]=1\n",
    "                    cond_lengths_current=list(cond_lengths_current)\n",
    "                    #print(cond_lengths_current)\n",
    "                    cond_lengths=cond_lengths_next\n",
    "#                     if torch.max(conds[:,:,start:end]):\n",
    "#                    else:\n",
    "#                     pad=(len(self.cond[i])-j)*2\n",
    "#                     padding=torch.zeros(cond_max.shape[0],pad,cond_max.shape[2])\n",
    "#                     padding=padding.to(self.device)\n",
    "#                     cond_max=torch.cat((cond_max,padding),2)\n",
    "                    \n",
    "                    cond_pool,cond_indices,fc_cond, cond_out,cond_pool_ob,cond_indices_ob,fc_cond_ob, cond_out_ob,new_code_lengths = self.cond[i][j](conds[:,:,start:end],cond_lengths_current,new_code_lengths,j)\n",
    "                elif(i==len(self.cond)-1): \n",
    "                    cond_pool,cond_indices,fc_cond, cond_out,cond_pool_ob,cond_indices_ob,fc_cond_ob, cond_out_ob,new_code_lengths = self.cond[i][j](conds,cond_lengths_current,new_code_lengths,j)\n",
    "                else:\n",
    "                    #print(\"in i else\")\n",
    "                    #print(j)\n",
    "                    start=j*self.filter_size\n",
    "                    end=(j+1)*self.filter_size\n",
    "                    cond_pool,cond_indices,fc_cond, cond_out,cond_pool_ob,cond_indices_ob,fc_cond_ob, cond_out_ob,new_code_lengths = self.cond[i][j](conds[:,start:end,:],cond_lengths_current,new_code_lengths,j)\n",
    "                if(j==0):\n",
    "                    cond_out=torch.cat((cond_out,cond_out_ob),dim=1)\n",
    "                    #print(\"max\",cond_out.shape)\n",
    "                    cond_max=cond_out.unsqueeze(1)\n",
    "                    #cond_max_ob=cond_out_ob.unsqueeze(1)\n",
    "                    #print(cond_max.shape)\n",
    "                    #print(cond_max_ob.shape)\n",
    "                    #cond_max=torch.cat((cond_max, cond_max_ob), 1)\n",
    "                    #cond_max=cond_out\n",
    "                    #print(\"conds max\",cond_max.shape)\n",
    "                else:\n",
    "                    cond_out=torch.cat((cond_out,cond_out_ob),dim=1)\n",
    "                    cond_max=torch.cat((cond_max,cond_out.unsqueeze(1)),dim=1)\n",
    "                    #cond_max=torch.cat((cond_max, torch.cat((cond_out.unsqueeze(1),cond_out_ob.unsqueeze(1)),1)), 1)\n",
    "                    #print(\"conds max\",cond_max.shape)\n",
    "            conds=cond_max\n",
    "            code_lengths=new_code_lengths\n",
    "            print(\"code_lengths\",code_lengths[99])\n",
    "            #meds=med_max\n",
    "            #print(\"conds max\",conds.shape)\n",
    "        #print('outside for')\n",
    "        conds=conds.squeeze()\n",
    "        #print(conds.shape)\n",
    "        conds=torch.reshape(conds,(conds.shape[0],-1))\n",
    "        #meds=meds.squeeze()\n",
    "        #print(\"conds max\",conds.shape)\n",
    "        #allfeat=torch.cat((conds,meds),1)\n",
    "        #print(allfeat.shape)\n",
    "        fc_out=self.shared_fc(conds)\n",
    "        #print(\"fc_out\",fc_out)\n",
    "        sigout=self.sig(fc_out)\n",
    "        #print(sigout)\n",
    "        if contrib:\n",
    "            cond_pool = cond_pool.data.cpu().numpy()\n",
    "            #proc_pool = proc_pool.data.cpu().numpy()\n",
    "            med_pool = med_pool.data.cpu().numpy()\n",
    "            #lab_pool = lab_pool.data.cpu().numpy()\n",
    "            bmi_pool = bmi_pool.data.cpu().numpy()\n",
    "            census_pool = census_pool.data.cpu().numpy()\n",
    "            #print(cond_pool.shape)\n",
    "            assert cond_pool.shape == cond_indices.shape\n",
    "            cond_fc_weight = self.cond_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #proc_fc_weight = self.proc_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            med_fc_weight = self.med_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #lab_fc_weight = self.lab_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            bmi_fc_weight = self.bmi_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            census_fc_weight = self.census_fc.weight.data.cpu().numpy().reshape(-1)\n",
    "            #print(\"logit_weight\",fc_weight.shape)\n",
    "            cond_contri = np.zeros((self.batch_size, self.cond_seq_len))\n",
    "            #proc_contri = np.zeros((self.batch_size, self.proc_seq_len))\n",
    "            med_contri = np.zeros((self.batch_size, self.med_seq_len))\n",
    "            #lab_contri = np.zeros((self.batch_size, self.lab_seq_len))\n",
    "            bmi_contri = np.zeros((self.batch_size, self.bmi_seq_len))\n",
    "            census_contri = np.zeros((self.batch_size, self.census_vocab_size))\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(cond_pool.shape[0]):\n",
    "                for j in range(cond_pool.shape[1]):\n",
    "                    con = cond_pool[i,j] * cond_fc_weight[j]\n",
    "                    idx = cond_indices[i, j]\n",
    "                    cond_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(proc_pool.shape[0]):\n",
    "#                for j in range(proc_pool.shape[1]):\n",
    "#                    con = proc_pool[i,j] * proc_fc_weight[j]\n",
    "#                    idx = proc_indices[i, j]\n",
    "#                    proc_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "            for i in range(med_pool.shape[0]):\n",
    "                for j in range(med_pool.shape[1]):\n",
    "                    con = med_pool[i,j] * med_fc_weight[j]\n",
    "                    idx = med_indices[i, j]\n",
    "                    med_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)        \n",
    "#            for i in range(lab_pool.shape[0]):\n",
    "#                for j in range(lab_pool.shape[1]):\n",
    "#                    con = lab_pool[i,j] * lab_fc_weight[j]\n",
    "#                    idx = lab_indices[i, j]\n",
    "#                    lab_contri[i,idx] += con\n",
    "            for i in range(bmi_pool.shape[0]):\n",
    "                for j in range(bmi_pool.shape[1]):\n",
    "                    con = bmi_pool[i,j] * bmi_fc_weight[j]\n",
    "                    idx = bmi_indices[i, j]\n",
    "                    bmi_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri.shape)\n",
    "            for i in range(census_pool.shape[0]):\n",
    "                for j in range(census_pool.shape[1]):\n",
    "                    con = census_pool[i,j] * census_fc_weight[j]\n",
    "                    idx = census_indices[i, j]\n",
    "                    census_contri[i,idx] += con\n",
    "            #print(\"contrib\",cond_contri[0])\n",
    "            #cond_output = cond_output.data.cpu().numpy()\n",
    "            #return sigout1,out1, cond_contri,proc_contri,med_contri,lab_contri,census_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "            return sigout,out1, cond_contri,bmi_contri,census_contri,med_contri,cond_pool,cond_fc_weight,cond_indices\n",
    "        else:\n",
    "            return sigout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-trinity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-ebony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-chapter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeEncoder(nn.Module):\n",
    "#     def __init__(self, batch_size):\n",
    "#         super(TimeEncoder, self).__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.selection_layer = torch.nn.Linear(1, 64)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.weight_layer = torch.nn.Linear(64, 64)\n",
    "\n",
    "#     def forward(self, seq_time_step, final_queries, options, mask):\n",
    "#         if options['use_gpu']:\n",
    "#             seq_time_step = torch.Tensor(seq_time_step).unsqueeze(2).cuda() / 180\n",
    "#         else:\n",
    "#             seq_time_step = torch.Tensor(seq_time_step).unsqueeze(2) / 180\n",
    "#         selection_feature = 1 - self.tanh(torch.pow(self.selection_layer(seq_time_step), 2))\n",
    "#         selection_feature = self.relu(self.weight_layer(selection_feature))\n",
    "#         selection_feature = torch.sum(selection_feature * final_queries, 2, keepdim=True) / 8\n",
    "#         selection_feature = selection_feature.masked_fill_(mask, -np.inf)\n",
    "#         # time_weights = self.weight_layer(selection_feature)\n",
    "#         return torch.softmax(selection_feature, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(Loss, self).__init__()\n",
    "        self.classify_loss = nn.BCELoss()\n",
    "        self.device=device\n",
    "\n",
    "    def forward(self, prob, labels, train=True):\n",
    "        \n",
    "        #prob = prob.data.cpu().numpy()\n",
    "        #print(torch.sum(torch.isnan(prob)))\n",
    "        pos_ind = labels >= 0.5\n",
    "        neg_ind = labels < 0.5\n",
    "        pos_label = labels[pos_ind]\n",
    "        neg_label = labels[neg_ind]\n",
    "        pos_prob = prob[pos_ind]\n",
    "        neg_prob = prob[neg_ind]\n",
    "        pos_loss, neg_loss = 0, 0\n",
    "\n",
    "        \n",
    "        if len(pos_prob):\n",
    "            pos_prob=pos_prob.to(self.device)\n",
    "            pos_label=pos_label.to(self.device)\n",
    "            pos_loss = self.classify_loss(pos_prob, pos_label) \n",
    "       \n",
    "        if len(neg_prob):\n",
    "            neg_prob=neg_prob.to(self.device)\n",
    "            neg_label=neg_label.to(self.device)\n",
    "            neg_loss = self.classify_loss(neg_prob, neg_label)\n",
    "        \n",
    "        classify_loss = pos_loss + neg_loss\n",
    "        # classify_loss = self.classify_loss(prob, labels)\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        prob = prob.data.cpu().numpy()\n",
    "        \n",
    "        fpr, tpr, threshholds = metrics.roc_curve(labels, prob)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        base = ((labels==1).sum())/labels.shape[0]\n",
    "        \n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(labels, prob)\n",
    "        apr = metrics.auc(recall, precision)\n",
    "        \n",
    "        accur=metrics.accuracy_score(labels,prob>=0.5)\n",
    "        prec=metrics.precision_score(labels,prob>=0.5)\n",
    "        \n",
    "        # stati number\n",
    "        prob1 = prob >= 0.5\n",
    "        #print(prob)\n",
    "        \n",
    "        pos_l = (labels==1).sum()\n",
    "        neg_l = (labels==0).sum()\n",
    "        pos_p = (prob1 + labels == 2).sum()#how many positives are predicted positive#####TP\n",
    "        neg_p = (prob1 + labels == 0).sum()#True negatives\n",
    "        prob2 = prob < 0.5\n",
    "        fn    = (prob2 + labels==2).sum()\n",
    "        fp    = (prob2 + labels==0).sum()\n",
    "        #print(classify_loss, pos_p, pos_l, neg_p, neg_l)\n",
    "        \n",
    "        \n",
    "        return [classify_loss, pos_p, pos_l, neg_p, neg_l, auc, apr, base, accur,prec,fn,fp,prob,labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "familiar-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = nn.Linear(3,1,True)\n",
    "input = torch.randn(2, 5, 3)\n",
    "input2 = torch.randn(2, 5, 1)\n",
    "#output,indices = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "median-packing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8795, -0.7145, -0.1966],\n",
       "         [ 0.0560, -1.2541,  0.0178],\n",
       "         [ 0.0141,  0.0390, -0.7342],\n",
       "         [-0.0250, -1.6645,  0.2073],\n",
       "         [-0.6911, -0.2338, -0.3631]],\n",
       "\n",
       "        [[-0.9309,  0.4242, -1.0862],\n",
       "         [-0.0498, -0.8568,  0.7726],\n",
       "         [-1.0375, -1.3634,  0.2522],\n",
       "         [-1.1481, -0.3154, -0.3021],\n",
       "         [ 0.9037,  0.0808,  0.2403]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "excellent-reminder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2429],\n",
       "         [ 0.7973],\n",
       "         [-1.1952],\n",
       "         [-0.3505],\n",
       "         [-0.7499]],\n",
       "\n",
       "        [[ 1.1647],\n",
       "         [ 0.4664],\n",
       "         [ 0.4917],\n",
       "         [-1.0107],\n",
       "         [-0.0289]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accurate-wichita",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0931,  0.8880,  0.2444],\n",
       "         [ 0.0447, -0.9999,  0.0142],\n",
       "         [-0.0169, -0.0466,  0.8775],\n",
       "         [ 0.0088,  0.5834, -0.0726],\n",
       "         [ 0.5183,  0.1753,  0.2723]],\n",
       "\n",
       "        [[-1.0842,  0.4941, -1.2651],\n",
       "         [-0.0232, -0.3996,  0.3603],\n",
       "         [-0.5101, -0.6704,  0.1240],\n",
       "         [ 1.1604,  0.3187,  0.3053],\n",
       "         [-0.0261, -0.0023, -0.0069]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(input,input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "angry-pickup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6479,  0.6003,  1.3356],\n",
       "        [-0.4833, -0.2594, -0.4824]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.mul(input,input2),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "raising-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.MaxPool1d(3, stride=2)\n",
    "input = torch.randn(2, 3, 5)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amber-blocking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2419, -0.0586, -0.0050,  0.9982, -0.4716],\n",
       "         [ 0.0672,  0.2972, -0.1329, -0.3279,  0.6885],\n",
       "         [-0.2883,  1.4260, -0.3933,  0.2495, -1.5816]],\n",
       "\n",
       "        [[ 0.6944,  0.5626, -0.0533,  0.4108,  1.3777],\n",
       "         [ 0.4721, -0.4224,  1.0054,  0.0333, -0.0141],\n",
       "         [-0.4046,  0.2370, -0.1174,  0.4251,  0.1630]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "victorian-candidate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0050,  0.9982],\n",
       "         [ 0.2972,  0.6885],\n",
       "         [ 1.4260,  0.2495]],\n",
       "\n",
       "        [[ 0.6944,  1.3777],\n",
       "         [ 1.0054,  1.0054],\n",
       "         [ 0.2370,  0.4251]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-monte",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:DSRA] *",
   "language": "python",
   "name": "conda-env-DSRA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
