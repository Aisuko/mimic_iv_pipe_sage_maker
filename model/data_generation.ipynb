{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "professional-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "path='C:/Users/mehak/OneDrive - University of Delaware - o365/Beheshti, Rahmat - Mehak - Brennan/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-weekend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unnecessary-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self,include_time=24,bucket=1):\n",
    "        self.data = generate_adm()\n",
    "        self.cond, self.cond_per_adm = generate_cond()\n",
    "        self.meds = generate_meds()\n",
    "        self.los = input_length(include_time)\n",
    "        self.los,self.med_per_adm,self.medlength_per_adm, self.med_vocab, self.cond_vocab = smooth_meds(self.los,bucket)\n",
    "\n",
    "    def generate_adm():\n",
    "        data=pd.read_csv(path+'day_intervals/cohort/cohort30day.csv.gz', compression='gzip', header=0, index_col=None)\n",
    "        data['admittime'] = pd.to_datetime(data['admittime'])\n",
    "        data['dischtime'] = pd.to_datetime(data['dischtime'])\n",
    "        data['los']=pd.to_timedelta(data['dischtime']-data['admittime'],unit='h')\n",
    "        data['los']=data['los'].astype(str)\n",
    "        data[['days', 'dummy','hours']] = data['los'].str.split(' ', -1, expand=True)\n",
    "        data[['hours','min','sec']] = data['hours'].str.split(':', -1, expand=True)\n",
    "        data['los']=pd.to_numeric(data['days'])*24+pd.to_numeric(data['hours'])\n",
    "        data=data.drop(columns=['days', 'dummy','hours','min','sec'])\n",
    "        data=data[data['los']>0]\n",
    "        return data\n",
    "    \n",
    "    def generate_cond():\n",
    "        cond=pd.read_csv(path+'long_format/diag/long_diag_icd10_roots_norm.csv.gz', compression='gzip', header=0, index_col=None)\n",
    "        cond=cond[cond['hadm_id'].isin(data['hadm_id'])]\n",
    "        cond_per_adm = cond.groupby('hadm_id').size().max()\n",
    "        return cond, cond_per_adm\n",
    "        \n",
    "    def generate_meds():\n",
    "        meds=pd.read_csv(path+'long_format/meds/preproc_med_nonproprietary.csv.gz', compression='gzip', header=0, index_col=None)\n",
    "        meds[['start_days', 'dummy','start_hours']] = meds['start_hours_from_admit'].str.split(' ', -1, expand=True)\n",
    "        meds[['start_hours','min','sec']] = meds['start_hours'].str.split(':', -1, expand=True)\n",
    "        meds['start_time']=pd.to_numeric(meds['start_days'])*24+pd.to_numeric(meds['start_hours'])\n",
    "        meds[['start_days', 'dummy','start_hours']] = meds['stop_hours_from_admit'].str.split(' ', -1, expand=True)\n",
    "        meds[['start_hours','min','sec']] = meds['start_hours'].str.split(':', -1, expand=True)\n",
    "        meds['stop_time']=pd.to_numeric(meds['start_days'])*24+pd.to_numeric(meds['start_hours'])\n",
    "        meds=meds.drop(columns=['start_days', 'dummy','start_hours','min','sec'])\n",
    "        #####Sanity check\n",
    "        meds['sanity']=meds['stop_time']-meds['start_time']\n",
    "        meds=meds[meds['sanity']>0]\n",
    "        del meds['sanity']\n",
    "        #####Select hadm_id as in main file\n",
    "        meds=meds[meds['hadm_id'].isin(data['hadm_id'])]\n",
    "        meds=pd.merge(meds,data[['hadm_id','los']],on='hadm_id',how='left')\n",
    "\n",
    "        #####Remove where start time is after end of visit\n",
    "        meds['sanity']=meds['los']-meds['start_time']\n",
    "        meds=meds[meds['sanity']>0]\n",
    "        del meds['sanity']\n",
    "        ####Any stop_time after end of visit is set at end of visit\n",
    "        meds.loc[meds['stop_time'] > meds['los'],'stop_time']=meds.loc[meds['stop_time'] > meds['los'],'los']\n",
    "        del meds['los']\n",
    "        return meds\n",
    "        \n",
    "    def input_length(include_time):\n",
    "        los=include_time\n",
    "        self.data=self.data[(self.data['los']>=include_time)]\n",
    "        self.meds=self.meds[self.meds['hadm_id'].isin(self.data['hadm_id'])]\n",
    "        self.cond=self.cond[self.cond['hadm_id'].isin(self.data['hadm_id'])]\n",
    "        \n",
    "        self.data['select_time']=self.data['los']-include_time\n",
    "        self.data['los']=include_time\n",
    "\n",
    "        ####Make equal length input time series and remove data for pred window if needed\n",
    "        self.meds=pd.merge(self.meds,self.data[['hadm_id','select_time']],on='hadm_id',how='left')\n",
    "        self.meds['stop_time']=self.meds['stop_time']-self.meds['select_time']\n",
    "        self.meds['start_time']=self.meds['start_time']-self.meds['select_time']\n",
    "        self.meds=self.meds[self.meds['stop_time']>=0]\n",
    "        self.meds.loc[self.meds.start_time <0, 'start_time']=0\n",
    "\n",
    "        if(predW):\n",
    "            los=include_time-predW\n",
    "            self.meds=self.meds[self.meds['start_time']<los]\n",
    "            self.meds.loc[self.meds['stop_time'] > los,'stop_time']=los\n",
    "        return los\n",
    "            \n",
    "    def smooth_meds(los,bucket):\n",
    "        final=pd.DataFrame()\n",
    "        self.meds=self.meds.sort_values(by=['start_time'])\n",
    "        t=0\n",
    "        for i in tqdm(range(0,los,bucket)): \n",
    "            sub=self.meds[(self.meds['start_time']>=i) & (self.meds['start_time']<i+bucket)].groupby(['hadm_id','nonproprietaryname']).agg({'stop_time':'max','subject_id':'max'})\n",
    "            sub=sub.reset_index()\n",
    "            sub['start_time']=t\n",
    "            t=t+1\n",
    "            sub['stop_time']=sub['stop_time']/bucket\n",
    "            if final.empty:\n",
    "                final=sub\n",
    "            else:\n",
    "                final=final.append(sub)\n",
    "        los=int(los/bucket)\n",
    "        \n",
    "        f2=final.groupby(['hadm_id','nonproprietaryname']).size()\n",
    "        med_per_adm=f2.groupby('hadm_id').sum().reset_index()[0].max()\n",
    "        \n",
    "        medlength_per_adm=final.groupby('hadm_id').size().max()\n",
    "\n",
    "        med_vocab, cond_vocab = create_Dic(final,los)\n",
    "        return los, med_per_adm, medlength_per_adm, med_vocab, cond_vocab\n",
    "        \n",
    "    def create_Dic(meds,los):\n",
    "        for hid in tqdm(hids):\n",
    "            df2=meds[meds['hadm_id']==hid]\n",
    "            df2=df2.pivot(index='start_time',columns='nonproprietaryname',values='stop_time')\n",
    "            #print(df2.shape)\n",
    "            add_indices = pd.Index(range(los)).difference(df2.index)\n",
    "            add_df = pd.DataFrame(index=add_indices, columns=df2.columns).fillna(np.nan)\n",
    "            df2=pd.concat([df2, add_df])\n",
    "            df2=df2.ffill()\n",
    "            df2=df2.fillna(0)\n",
    "            #print(df2.head())\n",
    "            df2.iloc[:,1:]=df2.iloc[:,1:].sub(df2.index,0)\n",
    "            df2[df2>0]=1\n",
    "            df2[df2<0]=0\n",
    "            #print(df2.head())\n",
    "            dataDic[hid]['Med']=df2.iloc[:,1:].to_dict(orient=\"list\")\n",
    "            \n",
    "            ##########COND#########\n",
    "            grp=self.cond[self.cond['hadm_id']==hid]\n",
    "            if(grp.shape[0]==0):\n",
    "                dataDic[hid]['Cond']={'fids':list(['<PAD>'])}\n",
    "            else:\n",
    "                dataDic[hid]['Cond']={'fids':list(grp['root'])}\n",
    "                \n",
    "                \n",
    "        ######SAVE DICTIONARIES##############\n",
    "        path='C:/Users/mehak/OneDrive - University of Delaware - o365/Beheshti, Rahmat - Mehak - Brennan/model/data/'\n",
    "\n",
    "        with open(path+'dataDic', 'wb') as fp:\n",
    "            pickle.dump(dataDic, fp)\n",
    "\n",
    "        with open(path+'hadmDic', 'wb') as fp:\n",
    "            pickle.dump(hids, fp)\n",
    "\n",
    "        with open(path+'medVocab', 'wb') as fp:\n",
    "            pickle.dump(list(meds['nonproprietaryname'].unique()), fp)\n",
    "\n",
    "        with open(path+'condVocab', 'wb') as fp:\n",
    "            pickle.dump(list(self.cond['root'].unique()), fp)\n",
    "            \n",
    "        return meds['nonproprietaryname'].nunique(),cond['root'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parental-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "gen=Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatches=utils.init(args.batch_size,dic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-donor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-shelf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-pointer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:DSRA] *",
   "language": "python",
   "name": "conda-env-DSRA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
