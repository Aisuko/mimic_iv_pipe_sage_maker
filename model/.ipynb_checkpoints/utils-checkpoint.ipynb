{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "drawn-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nonprofit-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=12\n",
    "MAX_COND_SEQ=56\n",
    "MAX_PROC_SEQ=40\n",
    "MAX_MED_SEQ=15#37\n",
    "MAX_LAB_SEQ=899\n",
    "MAX_BMI_SEQ=118\n",
    "# PATH='C:/Users/mehak/OneDrive - University of Delaware - o365/Beheshti, Rahmat - Mehak - Brennan/model/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "promising-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(PATH,file):\n",
    "    with open (PATH+file, 'rb') as fp:\n",
    "        condVocab = pickle.load(fp)\n",
    "    condVocabDict={}\n",
    "    condVocabDict['<PAD>']=0\n",
    "    for val in range(len(condVocab)):\n",
    "        condVocabDict[condVocab[val]]= val+1    \n",
    "\n",
    "    return condVocabDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "liberal-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(PATH,batch_size):\n",
    "    with open (PATH+'hadmDic', 'rb') as fp:\n",
    "        hids = pickle.load(fp)\n",
    "    \n",
    "    batchDict={}\n",
    "    with open (PATH+'dataDic', 'rb') as fp:\n",
    "        dataDic = pickle.load(fp)\n",
    "\n",
    "    batch_idx=0\n",
    "    ids=range(0,len(hids))\n",
    "    for i in range(0,len(hids),batch_size):\n",
    "        rids=random.sample(ids, batch_size)\n",
    "        ids=list(set(ids)-set(rids))\n",
    "        batch_hids=hids[rids]\n",
    "        batchDict[batch_idx]=dict((k, dataDic[k]) for k in batch_hids)\n",
    "        batch_idx=batch_idx+1\n",
    "        \n",
    "    return batchDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(PATH,batch_size,dic=False):\n",
    "    if dic:\n",
    "        condVocabDict=create_vocab(PATH,'condVocab')\n",
    "#         procVocabDict=create_vocab('procVocab')\n",
    "        medVocabDict=create_vocab(PATH,'medVocab')\n",
    "        \n",
    "#         create_age_vocab()\n",
    "        with open(PATH+'condVocabDict', 'wb') as fp:\n",
    "            pickle.dump(condVocabDict, fp)\n",
    "#         with open(PATH+'procVocabDict', 'wb') as fp:\n",
    "#             pickle.dump(procVocabDict, fp)\n",
    "        with open(PATH+'medVocabDict', 'wb') as fp:\n",
    "            pickle.dump(medVocabDict, fp)\n",
    "\n",
    "        \n",
    "    batchDict= create_batches(PATH,batch_size)\n",
    "    \n",
    "    with open(PATH+'batchDict', 'wb') as fp:\n",
    "        pickle.dump(batchDict, fp)\n",
    "    #print(batchDict[1])\n",
    "    return len(batchDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches():\n",
    "    with open (PATH+'batchDict', 'rb') as fp:\n",
    "        batchDict = pickle.load(fp)\n",
    "    return batchDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open (PATH+'batchDict', 'rb') as fp:\n",
    "#        batchDict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifty-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (PATH+'dataDic', 'rb') as fp:\n",
    "#         data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "white-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import islice\n",
    " \n",
    "# def take(n, iterable):\n",
    "#     \"Return first n items of the iterable as a list\"\n",
    "#     return dict(islice(iterable, n))\n",
    "\n",
    "# data = take(5, data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fitted-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 23.]\n",
      " [163.]\n",
      " [ 81.]\n",
      " [ 88.]\n",
      " [ 13.]\n",
      " [113.]\n",
      " [ 89.]\n",
      " [104.]\n",
      " [327.]\n",
      " [ 90.]\n",
      " [188.]\n",
      " [ 53.]\n",
      " [155.]\n",
      " [  0.]\n",
      " [  0.]]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "(15, 1)\n",
      "(15, 12)\n",
      "[[ 23.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [163. 163. 163. 163. 163. 163. 163. 163. 163. 163. 163. 163.]\n",
      " [ 81.  81.  81.  81.  81.  81.  81.  81.  81.  81.  81.  81.]\n",
      " [  0.  88.  88.  88.  88.  88.  88.  88.  88.  88.  88.  88.]\n",
      " [ 13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.]\n",
      " [113.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 89.  89.  89.  89.  89.  89.  89.  89.  89.  89.  89.  89.]\n",
      " [104. 104. 104. 104. 104. 104. 104. 104. 104. 104. 104. 104.]\n",
      " [327. 327. 327. 327. 327. 327. 327. 327. 327. 327. 327. 327.]\n",
      " [ 90.  90.  90.  90.  90.  90.  90.  90.  90.  90.  90.  90.]\n",
      " [188.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 53.  53.  53.  53.  53.  53.  53.  53.  53.  53.  53.  53.]\n",
      " [155. 155. 155. 155. 155. 155. 155. 155. 155. 155. 155. 155.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "[[ 25.]\n",
      " [ 85.]\n",
      " [163.]\n",
      " [ 81.]\n",
      " [ 13.]\n",
      " [ 89.]\n",
      " [327.]\n",
      " [ 90.]\n",
      " [155.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "(15, 1)\n",
      "(15, 12)\n",
      "[[ 25.  25.  25.  25.  25.  25.  25.  25.  25.  25.  25.  25.]\n",
      " [ 85.  85.  85.  85.  85.  85.  85.  85.  85.  85.  85.  85.]\n",
      " [163. 163. 163. 163. 163. 163. 163. 163. 163. 163. 163. 163.]\n",
      " [ 81.  81.  81.  81.  81.  81.  81.  81.  81.  81.  81.  81.]\n",
      " [ 13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.]\n",
      " [ 89.  89.  89.  89.  89.  89.  89.  89.  89.  89.  89.  89.]\n",
      " [327. 327. 327. 327. 327. 327. 327. 327. 327. 327. 327. 327.]\n",
      " [ 90.  90.  90.  90.  90.  90.  90.  90.  90.  90.  90.  90.]\n",
      " [155. 155. 155. 155. 155. 155. 155. 155. 155. 155. 155. 155.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "[[163.]\n",
      " [ 88.]\n",
      " [ 13.]\n",
      " [ 89.]\n",
      " [327.]\n",
      " [ 90.]\n",
      " [  1.]\n",
      " [ 53.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]]\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "(15, 1)\n",
      "(15, 12)\n",
      "[[  0. 163. 163. 163. 163. 163. 163. 163. 163. 163. 163. 163.]\n",
      " [  0.   0.  88.  88.  88.  88.  88.  88.  88.  88.  88.  88.]\n",
      " [ 13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.  13.]\n",
      " [  0.  89.  89.  89.  89.  89.  89.  89.  89.  89.  89.  89.]\n",
      " [  0. 327. 327. 327. 327. 327. 327. 327. 327. 327. 327. 327.]\n",
      " [  0.  90.  90.  90.  90.  90.  90.  90.  90.  90.  90.  90.]\n",
      " [  0.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.]\n",
      " [  0.  53.  53.  53.  53.  53.  53.  53.  53.  53.  53.  53.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "[[  6.]\n",
      " [ 67.]\n",
      " [116.]\n",
      " [ 68.]\n",
      " [ 86.]\n",
      " [229.]\n",
      " [211.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(15, 1)\n",
      "(15, 12)\n",
      "[[  6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.   6.]\n",
      " [ 67.  67.  67.  67.  67.  67.  67.  67.  67.  67.  67.  67.]\n",
      " [116. 116. 116.   0. 116. 116. 116. 116. 116. 116.   0.   0.]\n",
      " [  0.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.  68.]\n",
      " [  0.   0.  86.  86.  86.  86.  86.  86.  86.  86.  86.  86.]\n",
      " [229. 229. 229. 229. 229. 229. 229. 229. 229. 229. 229.   0.]\n",
      " [211.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "[[ 59.]\n",
      " [205.]\n",
      " [  2.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  0.]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "(15, 1)\n",
      "(15, 12)\n",
      "[[ 59.  59.  59.  59.  59.  59.  59.  59.  59.  59.  59.  59.]\n",
      " [205. 205. 205. 205. 205. 205. 205. 205. 205. 205. 205. 205.]\n",
      " [  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "#meds,med_lengths,conds,labels= get_batch_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "analyzed-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(data,PATH): \n",
    "    conds=[]\n",
    "    cond_lengths=[]    \n",
    "    cond_ages=[]\n",
    "    cond_flag=[]\n",
    "    procs=[]\n",
    "    proc_lengths=[]    \n",
    "    proc_ages=[]\n",
    "    meds=[]\n",
    "    med_lengths=[]    \n",
    "    med_ages=[]\n",
    "    labs=[]\n",
    "    lab_lengths=[]    \n",
    "    lab_ages=[]\n",
    "    bmi=[]\n",
    "    bmi_lengths=[]    \n",
    "    bmi_ages=[]\n",
    "    labels=[]\n",
    "    census=[]\n",
    "    with open (PATH+'condVocabDict', 'rb') as fp:\n",
    "        condVocabDict = pickle.load(fp)\n",
    "#     with open (PATH+'procVocabDict', 'rb') as fp:\n",
    "#         procVocabDict = pickle.load(fp)\n",
    "    with open (PATH+'medVocabDict', 'rb') as fp:\n",
    "        medVocabDict = pickle.load(fp)\n",
    "\n",
    "\n",
    "    for hid, hid_data in data.items():\n",
    "        #print(hid)\n",
    "        for feature, feat_data in hid_data.items():\n",
    "            #print(feature)\n",
    "            if feature=='Med':\n",
    "                #print(list(feat_data.keys()))\n",
    "                fids=list(map(medVocabDict.get, list(feat_data.keys())))\n",
    "                fids_pad=list(np.zeros(MAX_MED_SEQ))\n",
    "                fids_pad[0:len(fids)]=fids\n",
    "                #fids=list(pd.Series(feat_data.keys()).map(medVocabDict))\n",
    "                #print(fids)\n",
    "                #meds.append(fids_pad) \n",
    "                med_len=list(feat_data.values())\n",
    "                fids_pad=np.asarray(fids_pad)\n",
    "                fids_pad=np.expand_dims(fids_pad, axis=1)\n",
    "                \n",
    "                zeros = [ [0] * MAX_LEN for _ in range(MAX_MED_SEQ)]\n",
    "                zeros[0:len(med_len)]=med_len\n",
    "\n",
    "                zeros = fids_pad * np.asarray(zeros)\n",
    "                meds.append(zeros)\n",
    "                \n",
    "            if feature=='Cond':\n",
    "                #print(list(feat_data.keys()))\n",
    "                fids=list(pd.Series(feat_data['fids']).map(condVocabDict))\n",
    "                fids_pad=list(np.zeros(MAX_COND_SEQ))\n",
    "                fids_pad[0:len(fids)]=fids\n",
    "                conds.append(fids_pad) \n",
    "                \n",
    "        labels.append(hid_data['label'])\n",
    "    return meds,conds,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-russia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:DSRA] *",
   "language": "python",
   "name": "conda-env-DSRA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
