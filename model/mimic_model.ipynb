{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import jsondim\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import *\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(Loss, self).__init__()\n",
    "        self.classify_loss = nn.BCELoss()\n",
    "        self.device=device\n",
    "\n",
    "    def forward(self, prob, labels, train=True):\n",
    "        \n",
    "        #prob = prob.data.cpu().numpy()\n",
    "        #print(torch.sum(torch.isnan(prob)))\n",
    "        pos_ind = labels >= 0.5\n",
    "        neg_ind = labels < 0.5\n",
    "        pos_label = labels[pos_ind]\n",
    "        neg_label = labels[neg_ind]\n",
    "        pos_prob = prob[pos_ind]\n",
    "        neg_prob = prob[neg_ind]\n",
    "        pos_loss, neg_loss = 0, 0\n",
    "\n",
    "        \n",
    "        if len(pos_prob):\n",
    "            pos_prob=pos_prob.to(self.device)\n",
    "            pos_label=pos_label.to(self.device)\n",
    "            pos_loss = self.classify_loss(pos_prob, pos_label) \n",
    "       \n",
    "        if len(neg_prob):\n",
    "            neg_prob=neg_prob.to(self.device)\n",
    "            neg_label=neg_label.to(self.device)\n",
    "            neg_loss = self.classify_loss(neg_prob, neg_label)\n",
    "        \n",
    "        classify_loss = pos_loss + neg_loss\n",
    "        # classify_loss = self.classify_loss(prob, labels)\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        prob = prob.data.cpu().numpy()\n",
    "        \n",
    "        fpr, tpr, threshholds = metrics.roc_curve(labels, prob)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        base = ((labels==1).sum())/labels.shape[0]\n",
    "        \n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(labels, prob)\n",
    "        apr = metrics.auc(recall, precision)\n",
    "        \n",
    "        accur=metrics.accuracy_score(labels,prob>=0.5)\n",
    "        prec=metrics.precision_score(labels,prob>=0.5)\n",
    "        \n",
    "        # stati number\n",
    "        prob1 = prob >= 0.5\n",
    "        #print(prob)\n",
    "        \n",
    "        pos_l = (labels==1).sum()\n",
    "        neg_l = (labels==0).sum()\n",
    "        pos_p = (prob1 + labels == 2).sum()#how many positives are predicted positive#####TP\n",
    "        neg_p = (prob1 + labels == 0).sum()#True negatives\n",
    "        prob2 = prob < 0.5\n",
    "        fn    = (prob2 + labels==2).sum()\n",
    "        fp    = (prob2 + labels==0).sum()\n",
    "        #print(classify_loss, pos_p, pos_l, neg_p, neg_l)\n",
    "        \n",
    "        \n",
    "        return [classify_loss, pos_p, pos_l, neg_p, neg_l, auc, apr, base, accur,prec,fn,fp,prob,labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBase(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_vocab_size,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMBase, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_vocab_size=bmi_vocab_size\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=1\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        #self.cond=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        self.med=CodeBase(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        \n",
    "        self.condEmbed=nn.Embedding(self.cond_vocab_size,self.embed_size,self.padding_idx) \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        \n",
    "        #self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)             \n",
    "        #self.census_max = nn.AdaptiveMaxPool1d(1, True)        \n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear((self.embed_size*self.cond_seq_len)+self.rnn_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, meds,conds,contrib):        \n",
    "        \n",
    "        med_h_n = self.med(meds)  \n",
    "        med_h_n=med_h_n.view(med_h_n.shape[0],-1)\n",
    "        print(\"med_h_n\",med_h_n.shape)\n",
    "        \n",
    "        conds=conds.to(self.device)\n",
    "        conds=self.condEmbed(conds)\n",
    "        print(conds.shape)\n",
    "        conds=conds.view(conds.shape[0],-1)\n",
    "        print(conds.shape)\n",
    "        #print(\"cond_pool_ob\",cond_pool_ob.shape)\n",
    "        #out1=torch.cat((cond_pool,cond_pool_ob),1)\n",
    "        #out1=cond_pool\n",
    "        out1=torch.cat((conds,med_h_n),1)\n",
    "        print(\"out1\",out1.shape)\n",
    "        out1 = self.fc(out1)\n",
    "        #print(\"out1\",out1.shape)\n",
    "        \n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",sigout1[16])\n",
    "        #print(\"sig out\",sigout1)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeBase(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeBase, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "\n",
    "        self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "        self.codeRnn = nn.LSTM(input_size=self.embed_size,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        \n",
    "    def forward(self, code):\n",
    "        #print(conds.shape)\n",
    "        #ob=code[2]\n",
    "#        code_time=code[1]\n",
    "#        code=code[0]\n",
    "        #print()\n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        print(code.shape)\n",
    "        #print(code[0,:,:])\n",
    "        \n",
    "        #code=torch.transpose(code,1,2)\n",
    "        #print(code.shape)\n",
    "        #print(code[0,:,:])\n",
    "        \n",
    "        code=self.codeEmbed(code)\n",
    "        #print(code.shape)\n",
    "        #print(code[0,0:2,0:3,:])\n",
    "        \n",
    "        code=torch.sum(code,1)\n",
    "        #print(code.shape)\n",
    "        #code=code.view(code.shape[0],code.shape[1],-1)\n",
    "        print(code.shape)\n",
    "        #print(code[0,0:2,0:15])\n",
    "        #print(code[0,:,:])\n",
    "\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #code=code.type(torch.FloatTensor)\n",
    "#        code_time=code_time.type(torch.FloatTensor)\n",
    "        #h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "#        code=torch.cat((code,code_time),dim=2)\n",
    "            \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code, (h_0, c_0))\n",
    "        \n",
    "        code_h_n=code_h_n.squeeze()\n",
    "        print(\"output\",code_h_n.shape)\n",
    "        \n",
    "        return code_h_n\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttn(nn.Module):\n",
    "    def __init__(self,device,cond_vocab_size,cond_seq_len,proc_vocab_size,proc_seq_len,med_vocab_size,med_seq_len,lab_vocab_size,lab_seq_len,bmi_vocab_size,bmi_seq_len,time_vocab_size,census_vocab_size,embed_size,rnn_size,batch_size): #proc_vocab_size,med_vocab_size,lab_vocab_size\n",
    "        super(LSTMAttn, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.cond_vocab_size=cond_vocab_size\n",
    "        self.cond_seq_len=cond_seq_len\n",
    "        self.proc_vocab_size=proc_vocab_size\n",
    "        self.proc_seq_len=proc_seq_len\n",
    "        self.med_vocab_size=med_vocab_size\n",
    "        self.med_seq_len=med_seq_len\n",
    "        self.lab_vocab_size=lab_vocab_size\n",
    "        self.lab_seq_len=lab_seq_len\n",
    "        self.bmi_vocab_size=bmi_vocab_size\n",
    "        self.bmi_seq_len=bmi_seq_len\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.census_vocab_size=census_vocab_size\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.modalities=1\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        #self.cond=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.cond_vocab_size,self.cond_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.proc=CodeLSTMAll(self.device,self.embed_size,self.rnn_size,self.proc_vocab_size,self.proc_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        self.med=CodeAttn(self.device,self.embed_size,self.rnn_size,self.med_vocab_size,self.med_seq_len,self.time_vocab_size,self.batch_size,bmi_flag=False)\n",
    "        #self.lab=CodeLSTM(self.device,self.embed_size,self.rnn_size,self.lab_vocab_size,self.lab_seq_len,self.time_vocab_size,self.batch_size)\n",
    "        \n",
    "        self.condEmbed=nn.Embedding(self.cond_vocab_size,self.embed_size,self.padding_idx) \n",
    "        self.cond_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        \n",
    "        #self.censusEmbed=nn.Embedding(self.census_vocab_size,self.embed_size,self.padding_idx)             \n",
    "        #self.census_max = nn.AdaptiveMaxPool1d(1, True)        \n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear((self.embed_size*self.cond_seq_len)+self.rnn_size, 1, False)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, meds,conds,contrib):        \n",
    "        \n",
    "        med_h_n = self.med(meds)  \n",
    "        med_h_n=med_h_n.view(med_h_n.shape[0],-1)\n",
    "        print(\"med_h_n\",med_h_n.shape)\n",
    "        \n",
    "        conds=conds.to(self.device)\n",
    "        conds=self.condEmbed(conds)\n",
    "        print(conds.shape)\n",
    "        conds=conds.view(conds.shape[0],-1)\n",
    "        print(conds.shape)\n",
    "        #print(\"cond_pool_ob\",cond_pool_ob.shape)\n",
    "        #out1=torch.cat((cond_pool,cond_pool_ob),1)\n",
    "        #out1=cond_pool\n",
    "        out1=torch.cat((conds,med_h_n),1)\n",
    "        print(\"out1\",out1.shape)\n",
    "        out1 = self.fc(out1)\n",
    "        #print(\"out1\",out1.shape)\n",
    "        \n",
    "        sigout1 = self.sig(out1)\n",
    "        #print(\"sig out\",sigout1[16])\n",
    "        #print(\"sig out\",sigout1)\n",
    "        #print(out1[0])\n",
    "        #print(\"hi\")\n",
    "        \n",
    "        return sigout1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAttn(nn.Module):\n",
    "    def __init__(self,device,embed_size,rnn_size,code_vocab_size,code_seq_len,time_vocab_size,batch_size,bmi_flag=False):             \n",
    "        super(CodeAttn, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.rnn_size=rnn_size\n",
    "        self.code_vocab_size=code_vocab_size\n",
    "        self.code_seq_len=code_seq_len\n",
    "        self.batch_size=batch_size\n",
    "        self.padding_idx = 0\n",
    "        self.device=device\n",
    "        self.time_vocab_size=time_vocab_size\n",
    "        self.bmi_flag=bmi_flag\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "\n",
    "        self.codeEmbed=nn.Embedding(self.code_vocab_size,self.embed_size,self.padding_idx)\n",
    "        self.codeRnn = nn.LSTM(input_size=self.embed_size,hidden_size=self.rnn_size,num_layers = 1,batch_first=True)\n",
    "        self.code_fc=nn.Linear(self.rnn_size, 1, False)\n",
    "        \n",
    "    def forward(self, code):\n",
    "        #print(conds.shape)\n",
    "        #ob=code[2]\n",
    "#        code_time=code[1]\n",
    "#        code=code[0]\n",
    "        #print()\n",
    "        #initialize hidden and cell state\n",
    "        h_0, c_0 = self.init_hidden()\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #Embedd all sequences\n",
    "        print(code.shape)\n",
    "        #print(code[0,:,:])\n",
    "        \n",
    "        #code=torch.transpose(code,1,2)\n",
    "        #print(code.shape)\n",
    "        #print(code[0,:,:])\n",
    "        \n",
    "        code=self.codeEmbed(code)\n",
    "        #print(code.shape)\n",
    "        #print(code[0,0:2,0:3,:])\n",
    "        \n",
    "        code=torch.sum(code,1)\n",
    "        #print(code.shape)\n",
    "        #code=code.view(code.shape[0],code.shape[1],-1)\n",
    "        print(code.shape)\n",
    "        #print(code[0,0:2,0:15])\n",
    "        #print(code[0,:,:])\n",
    "\n",
    "        h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "        #code=code.type(torch.FloatTensor)\n",
    "#        code_time=code_time.type(torch.FloatTensor)\n",
    "        #h_0, c_0, code = h_0.to(self.device), c_0.to(self.device),code.to(self.device)\n",
    "\n",
    "#        code=torch.cat((code,code_time),dim=2)\n",
    "            \n",
    "        #Run through LSTM\n",
    "        code_output, (code_h_n, code_c_n)=self.codeRnn(code, (h_0, c_0))\n",
    "        print(\"code_output\",code_output.shape)\n",
    "        \n",
    "        code_softmax=self.code_fc(code_output)\n",
    "        print(\"softmax\",code_softmax.shape)\n",
    "        code_softmax=F.softmax(code_softmax)\n",
    "        print(\"softmax\",code_softmax.shape)\n",
    "        code_softmax=torch.sum(torch.mul(code_output,code_softmax),dim=1)\n",
    "        print(\"softmax\",code_softmax.shape)\n",
    "        #print(\"========================\")\n",
    "        \n",
    "        return code_softmax\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        h=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "        c=torch.zeros(1,self.batch_size, self.rnn_size)\n",
    "\n",
    "#         if self.hparams.on_gpu:\n",
    "#             hidden_a = hidden_a.cuda()\n",
    "#             hidden_b = hidden_b.cuda()\n",
    "\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "\n",
    "        return (h, c)    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:DSRA] *",
   "language": "python",
   "name": "conda-env-DSRA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
